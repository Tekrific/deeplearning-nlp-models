

TBD


[2] **GloVe**:

[3] **Some VAE model for NLP** (topic modeling: https://s4sarath.github.io/2016/11/23/variational_autoenocder_for_Natural_Language_Processing)

[4] **Seq2Seq Model (encoder-decoder)**: <br> &nbsp;&nbsp;&nbsp;&nbsp; **Paper**: [Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078.pdf)  <br> &nbsp;&nbsp;&nbsp;&nbsp; **Results**: [here](models/seq2seq/README.md)

[5] **Attention (Transformer)**:  <br> &nbsp;&nbsp;&nbsp;&nbsp; **Paper**: [Attention is all you need](https://arxiv.org/abs/1706.03762)   <br> &nbsp;&nbsp;&nbsp;&nbsp; **Results**: [here](models/attention/README.md)

[6] **BERT (Google AI)**:  <br> &nbsp;&nbsp;&nbsp;&nbsp; **Paper**: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)   <br> &nbsp;&nbsp;&nbsp;&nbsp; **Results**: [here](models/bert/README.md)

[7] **ELMo (Allen Institute)** https://arxiv.org/abs/1802.05365

[8] **GPT-2 (OpenAI)** https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf

