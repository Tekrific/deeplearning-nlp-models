{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "gpt.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "cWqvLoZ5KHfW"
      },
      "source": [
        "# The GPT Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "JSPXhKfdKHfW"
      },
      "source": [
        "## Colab Setup\n",
        "\n",
        "You can skip this section if not running on Google's colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Ope1RifFKHfW"
      },
      "source": [
        "If running with GPUs, sanity check that the GPUs are enabled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "A67s8lkTKHfW"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "pnEYkjAXKHfW",
        "outputId": "a0f348fa-419d-453d-d25c-78df7859a561",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "vH8GgVrAKHfW"
      },
      "source": [
        "Ahould be True. If not, debug (Note: version of pytorch I used is not capatible with CUDA drivers on colab. Follow these instructions here explicitly)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "SdLxeccRKHfW",
        "outputId": "e355f8a0-0a09-4f9e-b4a0-7df68c4d9722",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "wn4MxAW3KHfW"
      },
      "source": [
        "This should be \"/content\" on Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "qKH_ORuxKHfX"
      },
      "source": [
        "First, if running from colab, you must install the package. (You may skip if you installed already)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "nlbkIL-uKHfX"
      },
      "source": [
        "!git clone --single-branch --branch colab https://github.com/will-thompson-k/deeplearning-nlp-models.git\n",
        "%cd deeplearning-nlp-models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "AWhgwdqEKHfX"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "LFk888iGKHfX"
      },
      "source": [
        "!python setup.py install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "adqvmbeTKHfX"
      },
      "source": [
        "## Imports\n",
        "\n",
        "Here are the packages we need to import."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "nNZ3Z82aKHfX"
      },
      "source": [
        "from nlpmodels.models import gpt\n",
        "from nlpmodels.utils import train,utils,gpt_sampler\n",
        "from nlpmodels.utils.elt import gpt_dataset\n",
        "from argparse import Namespace\n",
        "utils.set_seed_everywhere()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "WCJ2j5gjKHfX"
      },
      "source": [
        "## Language Model: WikiText2\n",
        "\n",
        "We will try to train our transformer model to learn how to predict the next word in torchtext WikiText2 database.\n",
        "I took the first 300k from the training set to reduce computation time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "LDIaCTagKHfX"
      },
      "source": [
        "### Hyper-parameters\n",
        "\n",
        "These are the data processing and model training hyper-parameters for this run. Note that we are running a smaller model\n",
        "than cited in the paper for fewer iterations. This is meant merely to demonstrate it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% Parameters\n"
        },
        "id": "JaQqbExKKHfX"
      },
      "source": [
        "args = Namespace(\n",
        "        # Model hyper-parameters\n",
        "        num_layers_per_stack=2,  # original value = 12\n",
        "        dim_model=12, #original value = 768\n",
        "        dim_ffn=48, # original value = 3072\n",
        "        num_heads=2, # original value = 12\n",
        "        block_size=64, # original value = 512, context window\n",
        "        dropout=0.1,\n",
        "        # Training hyper-parameters\n",
        "        num_epochs=10, #obviously super short\n",
        "        learning_rate=0.0,\n",
        "        batch_size=64, #original value = 64\n",
        "    )"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "pC43FCeVKHfX"
      },
      "source": [
        "train_loader, vocab = gpt_dataset.GPTDataset.get_training_dataloader(args)\n",
        "model = gpt.GPT(vocab_size = len(vocab),\n",
        "            num_layers_per_stack= args.num_layers_per_stack,\n",
        "            dim_model = args.dim_model,\n",
        "            dim_ffn = args.dim_ffn,\n",
        "            num_heads = args.num_heads,\n",
        "            block_size = args.block_size,\n",
        "            dropout = args.dropout)\n",
        "trainer = train.GPTTrainer(args,vocab.mask_index,model,train_loader,vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "N2z3sqRJKHfY"
      },
      "source": [
        "# Self-supervised training\n",
        "Now we will run the first step in GPT training process, where we train the model to\n",
        "maximize the objective\n",
        "\n",
        "```\n",
        "max p(x[k]|x[k-1],[k-2],...x[k-block_size])\n",
        "```.\n",
        "\n",
        "This is an unsupervised (more aptly described as \"self-supervised\") loss. After this model is trained,\n",
        "we can run then continue it onto another problem (can freeze layers to only continue training the top layers)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ESk2wyBUKHfY",
        "outputId": "34b56b8f-bc41-42ca-9987-8dcf604b5d1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "trainer.run()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 0]: 100%|██████████| 4688/4688 [03:13<00:00, 24.17it/s, loss=5.74]\n",
            "[Epoch 1]: 100%|██████████| 4688/4688 [03:13<00:00, 24.26it/s, loss=5.41]\n",
            "[Epoch 2]: 100%|██████████| 4688/4688 [03:12<00:00, 24.30it/s, loss=5.45]\n",
            "[Epoch 3]: 100%|██████████| 4688/4688 [03:12<00:00, 24.36it/s, loss=5.32]\n",
            "[Epoch 4]: 100%|██████████| 4688/4688 [03:12<00:00, 24.35it/s, loss=5.29]\n",
            "[Epoch 5]: 100%|██████████| 4688/4688 [03:12<00:00, 24.33it/s, loss=5.23]\n",
            "[Epoch 6]: 100%|██████████| 4688/4688 [03:12<00:00, 24.34it/s, loss=5.24]\n",
            "[Epoch 7]: 100%|██████████| 4688/4688 [03:12<00:00, 24.39it/s, loss=5.28]\n",
            "[Epoch 8]: 100%|██████████| 4688/4688 [03:12<00:00, 24.32it/s, loss=5.17]\n",
            "[Epoch 9]: 100%|██████████| 4688/4688 [03:11<00:00, 24.44it/s, loss=5.27]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "WLwTk8fSKHfY"
      },
      "source": [
        "Note that this model was run for a **very** short period of time. The goal is just to show how this works - you can\n",
        "play with the hyper-parameters as you see fit.\n",
        "We only ran for 1 epoch, on a much smaller model,\n",
        "with a smaller dataset than was suggested in the paper.\n",
        "\n",
        "Let's see if the output the model completes makes any sense."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "KBvFfYmuKHfY"
      },
      "source": [
        "# GPT Completes A Sequence\n",
        "\n",
        "In the spirit of Kaparthy's minGPT::play_char notebook, we can use a greedy_sampler to see how the model\n",
        "continues a sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu0yyBuwTV2q"
      },
      "source": [
        "from typing import Tuple \n",
        "from nlpmodels.utils.elt import gpt_batch\n",
        "\n",
        "def reformat_data(data: Tuple) -> gpt_batch.GPTBatch:\n",
        "  \"\"\"\n",
        "  Args:\n",
        "      data (Tuple): The tuples of LongTensors to be converted into a Batch object.\n",
        "  Returns:\n",
        "      GPTBatch object containing data for a given batch.\n",
        "  \"\"\"\n",
        "  # (batch,seq) shape tensors\n",
        "  source_integers, target_integers = data\n",
        "\n",
        "  device = 'cpu'\n",
        "  if torch.cuda.is_available():\n",
        "      device = torch.cuda.current_device()\n",
        "  # place data on the correct device\n",
        "  source_integers = source_integers.to(device) if source_integers is not None else source_integers\n",
        "  target_integers = target_integers.to(device) if target_integers is not None else target_integers\n",
        "\n",
        "  # return a batch object with src,src_mask,tgt,tgt_mask tensors\n",
        "  batch_data = gpt_batch.GPTBatch(source_integers,\n",
        "                                  target_integers,\n",
        "                                  0)\n",
        "\n",
        "  return batch_data"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "0Gbx7PbeKHfY",
        "outputId": "5c84e6f8-39df-4936-cc68-0c7f5455eacd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "prompt = \"ernest hemingway first novel , the sun also rises , \" \\\n",
        "         \"treats of certain of those younger americas concerning whom gertrude stein has remarked :\" \\\n",
        "         \" you are all a lost generation . this is the novel for which a keen appetite was stimulated by\" \\\n",
        "         \" mr . hemingway 's exciting volume of short stories. \" \\\n",
        "         \" the clear objectivity and the sustained intensity of the stories , \" \\\n",
        "         \"and their concentration upon action in the present moment, seemed to point to a failure to project \" \\\n",
        "         \"a novel in terms of the same method, yet a resort to any other method would have let down the \" \\\n",
        "         \"reader's expectations. it is a relief to find that the sun also rises maintains the same heightened , \" \\\n",
        "         \"intimate tangibility as the shorter narratives and does it in the same kind of weighted, quickening prose. \"\n",
        "prompt_tensor = torch.LongTensor([[vocab.lookup_token(s) for s in prompt.split(\" \")]])\n",
        "prompt_tensor_batch = reformat_data((prompt_tensor,None))\n",
        "steps = 64\n",
        "y_hat_indices = gpt_sampler.sampler(model=model, data=prompt_tensor_batch,\n",
        "                                          steps=steps,block_size=64,do_sample=True).src\n",
        "y_hat_tokens = ' '.join([vocab.lookup_index(int(idx)) for idx in y_hat_indices[0]])"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-78e9494e6703>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m y_hat_indices = gpt_sampler.sampler(model=model, data=prompt_tensor_batch,\n\u001b[0;32m---> 10\u001b[0;31m                                           steps=steps,block_size=64,do_sample=True).src\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0my_hat_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_hat_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deeplearning-nlp-models/nlpmodels/utils/gpt_sampler.py\u001b[0m in \u001b[0;36msampler\u001b[0;34m(model, data, steps, block_size, do_sample)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# append to the sequence and continue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: can't set attribute"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "rzendnrIKHfY"
      },
      "source": [
        "Here is the prompt it was provided, a review of Ernest Hemingway's novel, The Sun Also Rises:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9UysiahgKHfY",
        "outputId": "4d7eed8f-a406-4ee3-a85c-a232f262b6dd"
      },
      "source": [
        "for idx in range(0,len(prompt.split(\" \")),8):\n",
        "    print(\" \".join(prompt.split(\" \")[idx:idx+8]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ernest hemingway first novel , the sun also\n",
            "rises , treats of certain of those younger\n",
            "americas concerning whom gertrude stein has remarked :\n",
            "you are all a lost generation . this\n",
            "is the novel for which a keen appetite\n",
            "was stimulated by mr . hemingway 's exciting\n",
            "volume of short stories.  the clear objectivity\n",
            "and the sustained intensity of the stories ,\n",
            "and their concentration upon action in the present\n",
            "moment, seemed to point to a failure to\n",
            "project a novel in terms of the same\n",
            "method, yet a resort to any other method\n",
            "would have let down the reader's expectations. it\n",
            "is a relief to find that the sun\n",
            "also rises maintains the same heightened , intimate\n",
            "tangibility as the shorter narratives and does it\n",
            "in the same kind of weighted, quickening prose.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "-z8__hoUKHfY"
      },
      "source": [
        "Working off that sequence, here is how the model completed the next 64 words in this review:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "8FVIBgfRKHfY",
        "outputId": "bbaffd61-978e-4eb5-d573-07654903a0a0"
      },
      "source": [
        "for idx in range(0,len(y_hat_tokens.split(\" \")),8):\n",
        "    print(\" \".join(y_hat_tokens.split(\" \")[idx:idx+8]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<unk> were started to <unk> olajuwon has been\n",
            "bed at in polish jazz @-@ overall films\n",
            ". around his violent several name or from\n",
            "the community in some vessels , passengers is\n",
            "being to direct works gas nuclear works of\n",
            "relationships and he believes that who chose taken\n",
            "the nba party , germany with distinguished war\n",
            "and on a representation . kevin bull ,\n",
            "but\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "_uXI1BvcKHfY"
      },
      "source": [
        "Well, as expected... this doesn't make any sense really. Pockets of words make sense, but overall it does not.\n",
        "As the language model is trained further and using more parameters, we would expect the believability of this text to increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "DiADkaVAKHfY"
      },
      "source": [
        "# Supervised training\n",
        "\n",
        "Once the model is trained in the self-supervised phase, go forth and apply it to a different problem!"
      ]
    }
  ]
}