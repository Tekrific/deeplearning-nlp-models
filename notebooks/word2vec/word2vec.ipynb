{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "TuFB1te9QZi4"
      },
      "source": [
        "# Skip-gram in Action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "8xfHObtRQZi5"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DYyFNKVQsuU"
      },
      "source": [
        "First, if running from colab, you must install the package. (You may skip if you installed already)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7_3_DdKQwsI",
        "outputId": "f914d23f-d37a-46e6-ba72-f6b1dddbe3a5"
      },
      "source": [
        "!git clone https://github.com/will-thompson-k/deeplearning-nlp-models.git\n",
        "%cd deeplearning-nlp-models"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'deeplearning-nlp-models' already exists and is not an empty directory.\n",
            "/content/deeplearning-nlp-models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ3IPlXrQ93X",
        "outputId": "8938ce3d-98d0-4ec9-8c65-ea5d7e94a09b"
      },
      "source": [
        "!python setup.py install"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing deeplearning_nlp_models.egg-info/PKG-INFO\n",
            "writing dependency_links to deeplearning_nlp_models.egg-info/dependency_links.txt\n",
            "writing top-level names to deeplearning_nlp_models.egg-info/top_level.txt\n",
            "writing manifest file 'deeplearning_nlp_models.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/nlpmodels\n",
            "creating build/bdist.linux-x86_64/egg/nlpmodels/models\n",
            "creating build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer_blocks/gpt_decoder.py -> build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer_blocks/decoder.py -> build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer_blocks/encoder.py -> build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer_blocks/attention.py -> build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer_blocks/sublayers.py -> build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer_blocks/__init__.py -> build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer.py -> build/bdist.linux-x86_64/egg/nlpmodels/models\n",
            "copying build/lib/nlpmodels/models/__init__.py -> build/bdist.linux-x86_64/egg/nlpmodels/models\n",
            "copying build/lib/nlpmodels/models/gpt.py -> build/bdist.linux-x86_64/egg/nlpmodels/models\n",
            "copying build/lib/nlpmodels/models/text_cnn.py -> build/bdist.linux-x86_64/egg/nlpmodels/models\n",
            "copying build/lib/nlpmodels/models/word2vec.py -> build/bdist.linux-x86_64/egg/nlpmodels/models\n",
            "creating build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/gpt_sampler.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "creating build/bdist.linux-x86_64/egg/nlpmodels/utils/elt\n",
            "copying build/lib/nlpmodels/utils/elt/dataset.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils/elt\n",
            "copying build/lib/nlpmodels/utils/elt/gpt_dataset.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils/elt\n",
            "copying build/lib/nlpmodels/utils/elt/transformer_dataset.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils/elt\n",
            "copying build/lib/nlpmodels/utils/elt/gpt_batch.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils/elt\n",
            "copying build/lib/nlpmodels/utils/elt/transformer_batch.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils/elt\n",
            "copying build/lib/nlpmodels/utils/elt/__init__.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils/elt\n",
            "copying build/lib/nlpmodels/utils/elt/skipgram_dataset.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils/elt\n",
            "copying build/lib/nlpmodels/utils/elt/text_cnn_dataset.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils/elt\n",
            "copying build/lib/nlpmodels/utils/label_smoother.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/tokenizer.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/__init__.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/train.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/vocabulary.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/utils.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/optims.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/__init__.py -> build/bdist.linux-x86_64/egg/nlpmodels\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks/gpt_decoder.py to gpt_decoder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks/decoder.py to decoder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks/encoder.py to encoder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks/attention.py to attention.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks/sublayers.py to sublayers.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer.py to transformer.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/gpt.py to gpt.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/text_cnn.py to text_cnn.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/word2vec.py to word2vec.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/gpt_sampler.py to gpt_sampler.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/elt/dataset.py to dataset.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/elt/gpt_dataset.py to gpt_dataset.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/elt/transformer_dataset.py to transformer_dataset.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/elt/gpt_batch.py to gpt_batch.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/elt/transformer_batch.py to transformer_batch.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/elt/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/elt/skipgram_dataset.py to skipgram_dataset.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/elt/text_cnn_dataset.py to text_cnn_dataset.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/label_smoother.py to label_smoother.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/tokenizer.py to tokenizer.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/train.py to train.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/vocabulary.py to vocabulary.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/utils.py to utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/optims.py to optims.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/__init__.py to __init__.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying deeplearning_nlp_models.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying deeplearning_nlp_models.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying deeplearning_nlp_models.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying deeplearning_nlp_models.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating 'dist/deeplearning_nlp_models-1.0-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing deeplearning_nlp_models-1.0-py3.6.egg\n",
            "Removing /usr/local/lib/python3.6/dist-packages/deeplearning_nlp_models-1.0-py3.6.egg\n",
            "Copying deeplearning_nlp_models-1.0-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "deeplearning-nlp-models 1.0 is already the active version in easy-install.pth\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/deeplearning_nlp_models-1.0-py3.6.egg\n",
            "Processing dependencies for deeplearning-nlp-models==1.0\n",
            "Finished processing dependencies for deeplearning-nlp-models==1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "wgWIaSV7SJ-y",
        "outputId": "cd170a94-3a66-43be-ec4e-f7d8bba5d37e"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.7.0\n",
            "  Using cached https://files.pythonhosted.org/packages/b9/f9/224b3893ab11d83d47fde357a7dcc75f00ba219f34f3d15e06fe4cb62e05/torchtext-0.7.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting torch==1.6.0\n",
            "  Using cached https://files.pythonhosted.org/packages/38/53/914885a93a44b96c0dd1c36f36ff10afe341f091230aad68f7228d61db1e/torch-1.6.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting numpy==1.19.1\n",
            "  Using cached https://files.pythonhosted.org/packages/b1/9a/7d474ba0860a41f771c9523d8c4ea56b084840b5ca4092d96bdee8a3b684/numpy-1.19.1-cp36-cp36m-manylinux2010_x86_64.whl\n",
            "Collecting datasets==1.1.2\n",
            "  Using cached https://files.pythonhosted.org/packages/f0/f4/2a3d6aee93ae7fce6c936dda2d7f534ad5f044a21238f85e28f0b205adf0/datasets-1.1.2-py3-none-any.whl\n",
            "Collecting tqdm==4.49.0\n",
            "  Using cached https://files.pythonhosted.org/packages/73/d5/f220e0c69b2f346b5649b66abebb391df1a00a59997a7ccf823325bd7a3e/tqdm-4.49.0-py2.py3-none-any.whl\n",
            "Collecting sentencepiece\n",
            "  Using cached https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.7.0->-r requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0->-r requirements.txt (line 2)) (0.16.0)\n",
            "Collecting pyarrow>=0.17.1\n",
            "  Using cached https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets==1.1.2->-r requirements.txt (line 4)) (1.1.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets==1.1.2->-r requirements.txt (line 4)) (0.8)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets==1.1.2->-r requirements.txt (line 4)) (0.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from datasets==1.1.2->-r requirements.txt (line 4)) (3.0.12)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets==1.1.2->-r requirements.txt (line 4)) (0.70.11.1)\n",
            "Collecting xxhash\n",
            "  Using cached https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.7.0->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.7.0->-r requirements.txt (line 1)) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.7.0->-r requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.7.0->-r requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets==1.1.2->-r requirements.txt (line 4)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets==1.1.2->-r requirements.txt (line 4)) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.1.2->-r requirements.txt (line 4)) (1.15.0)\n",
            "\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.6.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, sentencepiece, tqdm, torch, torchtext, pyarrow, xxhash, datasets\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed datasets-1.1.2 numpy-1.19.1 pyarrow-2.0.0 sentencepiece-0.1.94 torch-1.6.0 torchtext-0.7.0 tqdm-4.49.0 xxhash-2.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "torch",
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPl37AC7Q7Ge"
      },
      "source": [
        "Here are the packages we need to import."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "R1xTK4WDQZi5"
      },
      "source": [
        "from nlpmodels.models import word2vec\n",
        "from nlpmodels.utils import utils, train\n",
        "from nlpmodels.utils.elt import skipgram_dataset\n",
        "from argparse import Namespace\n",
        "import torch\n",
        "utils.set_seed_everywhere()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "_FAdhDejQZi6"
      },
      "source": [
        "## Hyper-parameters\n",
        "\n",
        "These are the data processing, skip-gram, and model training hyper-parameters for this run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% Parameters\n"
        },
        "id": "IjnfEI2EQZi6"
      },
      "source": [
        "args = Namespace(\n",
        "    # skip gram data hyper-parameters\n",
        "    context_window_size = 5,\n",
        "    subsample_t = 10.e-15, # param for sub-sampling frequent words (10.e-5 suggested by paper)\n",
        "    # Model hyper-parameters\n",
        "    embedding_size = 300,\n",
        "    negative_sample_size= 20, # k examples to be used in negative sampling loss function\n",
        "    # Training hyper-parameters\n",
        "    num_epochs=100,\n",
        "    learning_rate=0.001,\n",
        "    batch_size = 8192,\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "p_RwySKWQZi6"
      },
      "source": [
        "## Get Data\n",
        "\n",
        "Call the function that grabs training data (via hugging faces) and a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% Get data\n"
        },
        "id": "F6YRdoMSQZi6",
        "outputId": "262f05fa-3b80-4aaf-9c93-f789a8ae99f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_dataloader, vocab = skipgram_dataset.SkipGramDataset.get_training_dataloader(args.context_window_size,\n",
        "                                                                                   args.subsample_t,\n",
        "                                                                                   args.batch_size)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% Variables\n"
        },
        "id": "NefAdy_3QZi7",
        "outputId": "54aab6e1-a3f8-4031-88af-8021fd51f0de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "\n",
        "print(f\"The gist: context_window_size = {args.context_window_size}, \"\n",
        "      f\"batch_size = {args.batch_size}, vocab_size = {vocab_size}, \"\n",
        "      f\"embedding_size = {args.embedding_size}, k = {args.negative_sample_size}, \"\n",
        "      f\"train_size = {len(train_dataloader.dataset)}\"\n",
        "      )"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The gist: context_window_size = 5, batch_size = 8192, vocab_size = 61811, embedding_size = 300, k = 20, train_size = 720036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "fCIfZnhkQZi7"
      },
      "source": [
        "## Training\n",
        "\n",
        "Here we build the model and call the trainer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% Setting up training the model\n"
        },
        "id": "Dr8YY7JiQZi7",
        "outputId": "abe5330a-3539-4c3b-da5e-26d0b8c57403",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "word_frequencies = torch.from_numpy(vocab.get_word_frequencies())\n",
        "model = word2vec.SkipGramNSModel(vocab_size, args.embedding_size, args.negative_sample_size,word_frequencies)\n",
        "trainer = train.Word2VecTrainer(args,model,train_dataloader)\n",
        "trainer.run()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 0]:  36%|███▋      | 32/88 [00:34<00:59,  1.07s/it, loss=12.7]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-76388b671613>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSkipGramNSModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative_sample_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_frequencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2VecTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/deeplearning-nlp-models/nlpmodels/utils/train.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;31m# step 4. back_prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;31m# step 5. use optimizer to take gradient step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "zw_Ee0hbQZi7"
      },
      "source": [
        "## Examine Similarity of Embeddings\n",
        "\n",
        "Now that we've trained our embeddings, let's see if the words that are clustered together make any sense.\n",
        "\n",
        "We will use cosine similarity to find the embeddings that are most similar in the embeddings space. This is one metric\n",
        "for similarity. Another popular metric is based on euclidean distance. To use that metric, check out pytorch's\n",
        "cdist() function. Also, can't speak highly enough of `spotify::annoy` package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% Get the embeddings\n"
        },
        "id": "x-_W3wOrQZi7"
      },
      "source": [
        "embeddings = model.get_embeddings()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "UY7Edm_vQZi7"
      },
      "source": [
        "### Computer\n",
        "\n",
        "Let's see the top 5 words associated with \"computer\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% computer similar words\n"
        },
        "id": "BSOnHyWBQZi7",
        "outputId": "d176753f-d6f8-433c-827b-797158f4fd31"
      },
      "source": [
        "utils.get_cosine_similar(\"computer\",vocab._token_to_idx,embeddings)[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('of', tensor(1.0000)),\n",
              " ('apple', tensor(1.0000)),\n",
              " ('israel', tensor(1.0000)),\n",
              " ('leader', tensor(1.0000)),\n",
              " ('game', tensor(1.0000))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "1dBWUJm5QZi7"
      },
      "source": [
        "### Market\n",
        "\n",
        "Let's see the top 5 words associated with \"market\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% market similar words\n"
        },
        "id": "JoKo6JySQZi7",
        "outputId": "82169e22-2e3f-4e87-a6b1-2af843f536da"
      },
      "source": [
        "utils.get_cosine_similar(\"market\",vocab._token_to_idx,embeddings)[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('investors', tensor(1.0000)),\n",
              " ('korea', tensor(1.0000)),\n",
              " ('out', tensor(1.0000)),\n",
              " ('israel', tensor(1.0000)),\n",
              " ('china', tensor(1.0000))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "FqB5GFUfQZi8"
      },
      "source": [
        "In this particular example, we sub-selected heavily so that our training set would be manageable.\n",
        "With a training_N = ~200k and vocab_size = ~60k, we might consider increasing  N >> p to improve our embeddings."
      ]
    }
  ]
}