{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "TuFB1te9QZi4"
      },
      "source": [
        "# Skip-gram in Action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "8xfHObtRQZi5"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DYyFNKVQsuU"
      },
      "source": [
        "First, if running from colab, you must install the package. (You may skip if you installed already)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7_3_DdKQwsI",
        "outputId": "2df4f1d2-c169-40c8-b372-ccad8f8694d1"
      },
      "source": [
        "!git clone https://github.com/will-thompson-k/deeplearning-nlp-models.git\n",
        "%cd deeplearning-nlp-models"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deeplearning-nlp-models'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/43)\u001b[K\rremote: Counting objects:   4% (2/43)\u001b[K\rremote: Counting objects:   6% (3/43)\u001b[K\rremote: Counting objects:   9% (4/43)\u001b[K\rremote: Counting objects:  11% (5/43)\u001b[K\rremote: Counting objects:  13% (6/43)\u001b[K\rremote: Counting objects:  16% (7/43)\u001b[K\rremote: Counting objects:  18% (8/43)\u001b[K\rremote: Counting objects:  20% (9/43)\u001b[K\rremote: Counting objects:  23% (10/43)\u001b[K\rremote: Counting objects:  25% (11/43)\u001b[K\rremote: Counting objects:  27% (12/43)\u001b[K\rremote: Counting objects:  30% (13/43)\u001b[K\rremote: Counting objects:  32% (14/43)\u001b[K\rremote: Counting objects:  34% (15/43)\u001b[K\rremote: Counting objects:  37% (16/43)\u001b[K\rremote: Counting objects:  39% (17/43)\u001b[K\rremote: Counting objects:  41% (18/43)\u001b[K\rremote: Counting objects:  44% (19/43)\u001b[K\rremote: Counting objects:  46% (20/43)\u001b[K\rremote: Counting objects:  48% (21/43)\u001b[K\rremote: Counting objects:  51% (22/43)\u001b[K\rremote: Counting objects:  53% (23/43)\u001b[K\rremote: Counting objects:  55% (24/43)\u001b[K\rremote: Counting objects:  58% (25/43)\u001b[K\rremote: Counting objects:  60% (26/43)\u001b[K\rremote: Counting objects:  62% (27/43)\u001b[K\rremote: Counting objects:  65% (28/43)\u001b[K\rremote: Counting objects:  67% (29/43)\u001b[K\rremote: Counting objects:  69% (30/43)\u001b[K\rremote: Counting objects:  72% (31/43)\u001b[K\rremote: Counting objects:  74% (32/43)\u001b[K\rremote: Counting objects:  76% (33/43)\u001b[K\rremote: Counting objects:  79% (34/43)\u001b[K\rremote: Counting objects:  81% (35/43)\u001b[K\rremote: Counting objects:  83% (36/43)\u001b[K\rremote: Counting objects:  86% (37/43)\u001b[K\rremote: Counting objects:  88% (38/43)\u001b[K\rremote: Counting objects:  90% (39/43)\u001b[K\rremote: Counting objects:  93% (40/43)\u001b[K\rremote: Counting objects:  95% (41/43)\u001b[K\rremote: Counting objects:  97% (42/43)\u001b[K\rremote: Counting objects: 100% (43/43)\u001b[K\rremote: Counting objects: 100% (43/43), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 869 (delta 21), reused 22 (delta 10), pack-reused 826\u001b[K\n",
            "Receiving objects: 100% (869/869), 3.60 MiB | 39.66 MiB/s, done.\n",
            "Resolving deltas: 100% (513/513), done.\n",
            "/content/deeplearning-nlp-models/deeplearning-nlp-models/deeplearning-nlp-models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ3IPlXrQ93X",
        "outputId": "db9dcbe6-fa1e-424c-e0cb-55ed6cd48f66"
      },
      "source": [
        "!python setup.py install"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating deeplearning_nlp_models.egg-info\n",
            "writing deeplearning_nlp_models.egg-info/PKG-INFO\n",
            "writing dependency_links to deeplearning_nlp_models.egg-info/dependency_links.txt\n",
            "writing top-level names to deeplearning_nlp_models.egg-info/top_level.txt\n",
            "writing manifest file 'deeplearning_nlp_models.egg-info/SOURCES.txt'\n",
            "writing manifest file 'deeplearning_nlp_models.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/nlpmodels\n",
            "copying nlpmodels/__init__.py -> build/lib/nlpmodels\n",
            "creating build/lib/nlpmodels/utils\n",
            "copying nlpmodels/utils/gpt_sampler.py -> build/lib/nlpmodels/utils\n",
            "copying nlpmodels/utils/label_smoother.py -> build/lib/nlpmodels/utils\n",
            "copying nlpmodels/utils/tokenizer.py -> build/lib/nlpmodels/utils\n",
            "copying nlpmodels/utils/__init__.py -> build/lib/nlpmodels/utils\n",
            "copying nlpmodels/utils/train.py -> build/lib/nlpmodels/utils\n",
            "copying nlpmodels/utils/vocabulary.py -> build/lib/nlpmodels/utils\n",
            "copying nlpmodels/utils/utils.py -> build/lib/nlpmodels/utils\n",
            "copying nlpmodels/utils/optims.py -> build/lib/nlpmodels/utils\n",
            "creating build/lib/nlpmodels/utils/elt\n",
            "copying nlpmodels/utils/elt/dataset.py -> build/lib/nlpmodels/utils/elt\n",
            "copying nlpmodels/utils/elt/gpt_dataset.py -> build/lib/nlpmodels/utils/elt\n",
            "copying nlpmodels/utils/elt/transformer_dataset.py -> build/lib/nlpmodels/utils/elt\n",
            "copying nlpmodels/utils/elt/gpt_batch.py -> build/lib/nlpmodels/utils/elt\n",
            "copying nlpmodels/utils/elt/transformer_batch.py -> build/lib/nlpmodels/utils/elt\n",
            "copying nlpmodels/utils/elt/__init__.py -> build/lib/nlpmodels/utils/elt\n",
            "copying nlpmodels/utils/elt/skipgram_dataset.py -> build/lib/nlpmodels/utils/elt\n",
            "copying nlpmodels/utils/elt/text_cnn_dataset.py -> build/lib/nlpmodels/utils/elt\n",
            "creating build/lib/nlpmodels/models\n",
            "copying nlpmodels/models/transformer.py -> build/lib/nlpmodels/models\n",
            "copying nlpmodels/models/__init__.py -> build/lib/nlpmodels/models\n",
            "copying nlpmodels/models/gpt.py -> build/lib/nlpmodels/models\n",
            "copying nlpmodels/models/text_cnn.py -> build/lib/nlpmodels/models\n",
            "copying nlpmodels/models/word2vec.py -> build/lib/nlpmodels/models\n",
            "creating build/lib/nlpmodels/models/transformer_blocks\n",
            "copying nlpmodels/models/transformer_blocks/gpt_decoder.py -> build/lib/nlpmodels/models/transformer_blocks\n",
            "copying nlpmodels/models/transformer_blocks/decoder.py -> build/lib/nlpmodels/models/transformer_blocks\n",
            "copying nlpmodels/models/transformer_blocks/encoder.py -> build/lib/nlpmodels/models/transformer_blocks\n",
            "copying nlpmodels/models/transformer_blocks/attention.py -> build/lib/nlpmodels/models/transformer_blocks\n",
            "copying nlpmodels/models/transformer_blocks/sublayers.py -> build/lib/nlpmodels/models/transformer_blocks\n",
            "copying nlpmodels/models/transformer_blocks/__init__.py -> build/lib/nlpmodels/models/transformer_blocks\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/nlpmodels\n",
            "creating build/bdist.linux-x86_64/egg/nlpmodels/models\n",
            "creating build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer_blocks/gpt_decoder.py -> build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer_blocks/decoder.py -> build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer_blocks/encoder.py -> build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer_blocks/attention.py -> build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer_blocks/sublayers.py -> build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer_blocks/__init__.py -> build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer.py -> build/bdist.linux-x86_64/egg/nlpmodels/models\n",
            "copying build/lib/nlpmodels/models/__init__.py -> build/bdist.linux-x86_64/egg/nlpmodels/models\n",
            "copying build/lib/nlpmodels/models/gpt.py -> build/bdist.linux-x86_64/egg/nlpmodels/models\n",
            "copying build/lib/nlpmodels/models/text_cnn.py -> build/bdist.linux-x86_64/egg/nlpmodels/models\n",
            "copying build/lib/nlpmodels/models/word2vec.py -> build/bdist.linux-x86_64/egg/nlpmodels/models\n",
            "creating build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/gpt_sampler.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "creating build/bdist.linux-x86_64/egg/nlpmodels/utils/elt\n",
            "copying build/lib/nlpmodels/utils/elt/dataset.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils/elt\n",
            "copying build/lib/nlpmodels/utils/elt/gpt_dataset.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils/elt\n",
            "copying build/lib/nlpmodels/utils/elt/transformer_dataset.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils/elt\n",
            "copying build/lib/nlpmodels/utils/elt/gpt_batch.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils/elt\n",
            "copying build/lib/nlpmodels/utils/elt/transformer_batch.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils/elt\n",
            "copying build/lib/nlpmodels/utils/elt/__init__.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils/elt\n",
            "copying build/lib/nlpmodels/utils/elt/skipgram_dataset.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils/elt\n",
            "copying build/lib/nlpmodels/utils/elt/text_cnn_dataset.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils/elt\n",
            "copying build/lib/nlpmodels/utils/label_smoother.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/tokenizer.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/__init__.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/train.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/vocabulary.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/utils.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/optims.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/__init__.py -> build/bdist.linux-x86_64/egg/nlpmodels\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks/gpt_decoder.py to gpt_decoder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks/decoder.py to decoder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks/encoder.py to encoder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks/attention.py to attention.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks/sublayers.py to sublayers.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer.py to transformer.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/gpt.py to gpt.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/text_cnn.py to text_cnn.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/word2vec.py to word2vec.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/gpt_sampler.py to gpt_sampler.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/elt/dataset.py to dataset.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/elt/gpt_dataset.py to gpt_dataset.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/elt/transformer_dataset.py to transformer_dataset.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/elt/gpt_batch.py to gpt_batch.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/elt/transformer_batch.py to transformer_batch.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/elt/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/elt/skipgram_dataset.py to skipgram_dataset.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/elt/text_cnn_dataset.py to text_cnn_dataset.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/label_smoother.py to label_smoother.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/tokenizer.py to tokenizer.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/train.py to train.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/vocabulary.py to vocabulary.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/utils.py to utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/optims.py to optims.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/__init__.py to __init__.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying deeplearning_nlp_models.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying deeplearning_nlp_models.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying deeplearning_nlp_models.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying deeplearning_nlp_models.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/deeplearning_nlp_models-1.0-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing deeplearning_nlp_models-1.0-py3.6.egg\n",
            "Removing /usr/local/lib/python3.6/dist-packages/deeplearning_nlp_models-1.0-py3.6.egg\n",
            "Copying deeplearning_nlp_models-1.0-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "deeplearning-nlp-models 1.0 is already the active version in easy-install.pth\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/deeplearning_nlp_models-1.0-py3.6.egg\n",
            "Processing dependencies for deeplearning-nlp-models==1.0\n",
            "Finished processing dependencies for deeplearning-nlp-models==1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "wgWIaSV7SJ-y",
        "outputId": "6c8a848a-45df-4586-f1c6-48bee08e7b78"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.7.0\n",
            "  Using cached https://files.pythonhosted.org/packages/b9/f9/224b3893ab11d83d47fde357a7dcc75f00ba219f34f3d15e06fe4cb62e05/torchtext-0.7.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting torch==1.6.0\n",
            "  Using cached https://files.pythonhosted.org/packages/38/53/914885a93a44b96c0dd1c36f36ff10afe341f091230aad68f7228d61db1e/torch-1.6.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting numpy==1.19.1\n",
            "  Using cached https://files.pythonhosted.org/packages/b1/9a/7d474ba0860a41f771c9523d8c4ea56b084840b5ca4092d96bdee8a3b684/numpy-1.19.1-cp36-cp36m-manylinux2010_x86_64.whl\n",
            "Collecting datasets==1.1.2\n",
            "  Using cached https://files.pythonhosted.org/packages/f0/f4/2a3d6aee93ae7fce6c936dda2d7f534ad5f044a21238f85e28f0b205adf0/datasets-1.1.2-py3-none-any.whl\n",
            "Collecting tqdm==4.49.0\n",
            "  Using cached https://files.pythonhosted.org/packages/73/d5/f220e0c69b2f346b5649b66abebb391df1a00a59997a7ccf823325bd7a3e/tqdm-4.49.0-py2.py3-none-any.whl\n",
            "Collecting sentencepiece\n",
            "  Using cached https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.7.0->-r requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0->-r requirements.txt (line 2)) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from datasets==1.1.2->-r requirements.txt (line 4)) (3.0.12)\n",
            "Collecting xxhash\n",
            "  Using cached https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl\n",
            "Collecting pyarrow>=0.17.1\n",
            "  Using cached https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets==1.1.2->-r requirements.txt (line 4)) (1.1.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets==1.1.2->-r requirements.txt (line 4)) (0.70.11.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets==1.1.2->-r requirements.txt (line 4)) (0.8)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets==1.1.2->-r requirements.txt (line 4)) (0.3.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.7.0->-r requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.7.0->-r requirements.txt (line 1)) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.7.0->-r requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.7.0->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets==1.1.2->-r requirements.txt (line 4)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets==1.1.2->-r requirements.txt (line 4)) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.1.2->-r requirements.txt (line 4)) (1.15.0)\n",
            "\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.6.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, torch, sentencepiece, tqdm, torchtext, xxhash, pyarrow, datasets\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed datasets-1.1.2 numpy-1.19.1 pyarrow-2.0.0 sentencepiece-0.1.94 torch-1.6.0 torchtext-0.7.0 tqdm-4.49.0 xxhash-2.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pyarrow",
                  "torch",
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPl37AC7Q7Ge"
      },
      "source": [
        "Here are the packages we need to import."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "R1xTK4WDQZi5",
        "outputId": "73614ec4-73a5-4256-dfd6-b78994fefbf2"
      },
      "source": [
        "from nlpmodels.models import word2vec\n",
        "from nlpmodels.utils import utils, train\n",
        "from nlpmodels.utils.elt import skipgram_dataset\n",
        "from argparse import Namespace\n",
        "import torch\n",
        "utils.set_seed_everywhere()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ZipImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZipImportError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b553676ad57f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnlpmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnlpmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnlpmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mskipgram_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0margparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNamespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZipImportError\u001b[0m: bad local file header: '/usr/local/lib/python3.6/dist-packages/deeplearning_nlp_models-1.0-py3.6.egg'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "_FAdhDejQZi6"
      },
      "source": [
        "## Hyper-parameters\n",
        "\n",
        "These are the data processing, skip-gram, and model training hyper-parameters for this run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% Parameters\n"
        },
        "id": "IjnfEI2EQZi6"
      },
      "source": [
        "args = Namespace(\n",
        "    # skip gram data hyper-parameters\n",
        "    context_window_size = 5,\n",
        "    subsample_t = 10.e-15, # param for sub-sampling frequent words (10.e-5 suggested by paper)\n",
        "    # Model hyper-parameters\n",
        "    embedding_size = 300,\n",
        "    negative_sample_size= 20, # k examples to be used in negative sampling loss function\n",
        "    # Training hyper-parameters\n",
        "    num_epochs=100,\n",
        "    learning_rate=0.0001,\n",
        "    batch_size = 4096,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "p_RwySKWQZi6"
      },
      "source": [
        "## Get Data\n",
        "\n",
        "Call the function that grabs training data (via hugging faces) and a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% Get data\n"
        },
        "id": "F6YRdoMSQZi6",
        "outputId": "c77c2926-3728-4d36-d5be-5120394d0ccd"
      },
      "source": [
        "train_dataloader, vocab = skipgram_dataset.SkipGramDataset.get_training_dataloader(args.context_window_size,\n",
        "                                                                                   args.subsample_t,\n",
        "                                                                                   args.batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% Variables\n"
        },
        "id": "NefAdy_3QZi7",
        "outputId": "2b676fc1-1f4f-4053-be83-014f92546bcc"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "\n",
        "print(f\"The gist: context_window_size = {args.context_window_size}, \"\n",
        "      f\"batch_size = {args.batch_size}, vocab_size = {vocab_size}, \"\n",
        "      f\"embedding_size = {args.embedding_size}, k = {args.negative_sample_size}, \"\n",
        "      f\"train_size = {len(train_dataloader.dataset)}\"\n",
        "      )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The gist: context_window_size = 5, batch_size = 4096, vocab_size = 61810, embedding_size = 300, k = 20, train_size = 240032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "fCIfZnhkQZi7"
      },
      "source": [
        "## Training\n",
        "\n",
        "Here we build the model and call the trainer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% Setting up training the model\n"
        },
        "id": "Dr8YY7JiQZi7",
        "outputId": "5b101fb8-9951-40de-8c51-d72e905344a0"
      },
      "source": [
        "word_frequencies = torch.from_numpy(vocab.get_word_frequencies())\n",
        "model = word2vec.SkipGramNSModel(vocab_size, args.embedding_size, args.negative_sample_size,word_frequencies)\n",
        "trainer = train.Word2VecTrainer(args,model,train_dataloader)\n",
        "trainer.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 0]: 100%|██████████| 59/59 [01:04<00:00,  1.09s/it, loss=14.5]\n",
            "[Epoch 99]: 100%|██████████| 59/59 [00:50<00:00,  1.16it/s, loss=0.744]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "zw_Ee0hbQZi7"
      },
      "source": [
        "## Examine Similarity of Embeddings\n",
        "\n",
        "Now that we've trained our embeddings, let's see if the words that are clustered together make any sense.\n",
        "\n",
        "We will use cosine similarity to find the embeddings that are most similar in the embeddings space. This is one metric\n",
        "for similarity. Another popular metric is based on euclidean distance. To use that metric, check out pytorch's\n",
        "cdist() function. Also, can't speak highly enough of `spotify::annoy` package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% Get the embeddings\n"
        },
        "id": "x-_W3wOrQZi7"
      },
      "source": [
        "embeddings = model.get_embeddings()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "UY7Edm_vQZi7"
      },
      "source": [
        "### Computer\n",
        "\n",
        "Let's see the top 5 words associated with \"computer\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% computer similar words\n"
        },
        "id": "BSOnHyWBQZi7",
        "outputId": "d176753f-d6f8-433c-827b-797158f4fd31"
      },
      "source": [
        "utils.get_cosine_similar(\"computer\",vocab._token_to_idx,embeddings)[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('of', tensor(1.0000)),\n",
              " ('apple', tensor(1.0000)),\n",
              " ('israel', tensor(1.0000)),\n",
              " ('leader', tensor(1.0000)),\n",
              " ('game', tensor(1.0000))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "1dBWUJm5QZi7"
      },
      "source": [
        "### Market\n",
        "\n",
        "Let's see the top 5 words associated with \"market\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% market similar words\n"
        },
        "id": "JoKo6JySQZi7",
        "outputId": "82169e22-2e3f-4e87-a6b1-2af843f536da"
      },
      "source": [
        "utils.get_cosine_similar(\"market\",vocab._token_to_idx,embeddings)[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('investors', tensor(1.0000)),\n",
              " ('korea', tensor(1.0000)),\n",
              " ('out', tensor(1.0000)),\n",
              " ('israel', tensor(1.0000)),\n",
              " ('china', tensor(1.0000))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "FqB5GFUfQZi8"
      },
      "source": [
        "In this particular example, we sub-selected heavily so that our training set would be manageable.\n",
        "With a training_N = ~200k and vocab_size = ~60k, we might consider increasing  N >> p to improve our embeddings."
      ]
    }
  ]
}