{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "TuFB1te9QZi4"
      },
      "source": [
        "# Skip-gram in Action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "8xfHObtRQZi5"
      },
      "source": [
        "## Colab Setup\n",
        "\n",
        "You can skip this section if not running on Google's colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KATZhWEKdvCt"
      },
      "source": [
        "If running with GPUs, sanity check that the GPUs are enabled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbMGzCvnhLoB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb67f3ff-2f5a-4038-f2ee-9c2710bef7df"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Dec  1 13:13:01 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ4c_IP5dt6K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00102c54-dacd-424c-a9c3-3ffea1bdb6d9"
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blhzRQd4icDp"
      },
      "source": [
        "The above should be True. If not, debug (Note: version of pytorch I used is not capatible with CUDA drivers on colab. Follow these instructions here explicitly)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DYyFNKVQsuU"
      },
      "source": [
        "First, if running from colab, you must install the package. (You may skip if you installed already)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7_3_DdKQwsI",
        "outputId": "9d422e2b-0b0b-443e-bc13-dab0a1ac246d"
      },
      "source": [
        "!git clone --single-branch --branch colab https://github.com/will-thompson-k/deeplearning-nlp-models.git\n",
        "%cd deeplearning-nlp-models"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deeplearning-nlp-models'...\n",
            "remote: Enumerating objects: 88, done.\u001b[K\n",
            "remote: Counting objects: 100% (88/88), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 914 (delta 48), reused 34 (delta 17), pack-reused 826\u001b[K\n",
            "Receiving objects: 100% (914/914), 3.63 MiB | 3.07 MiB/s, done.\n",
            "Resolving deltas: 100% (540/540), done.\n",
            "/content/deeplearning-nlp-models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgWIaSV7SJ-y",
        "outputId": "823bd9cd-3f6a-4a46-ccab-520d7d79cff6"
      },
      "source": [
        "!pip install datasets==1.0.2"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets==1.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/7e/8d9e2fd30e3819e6042927d379f3668a0b49fe38b92d5639194808a1d877/datasets-1.0.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets==1.0.2) (0.3.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets==1.0.2) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from datasets==1.0.2) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets==1.0.2) (0.8)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets==1.0.2) (1.18.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets==1.0.2) (2.23.0)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets==1.0.2) (2.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets==1.0.2) (1.1.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets==1.0.2) (4.41.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets==1.0.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets==1.0.2) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets==1.0.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets==1.0.2) (2.10)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets==1.0.2) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets==1.0.2) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.0.2) (1.15.0)\n",
            "Installing collected packages: datasets\n",
            "  Found existing installation: datasets 1.1.3\n",
            "    Uninstalling datasets-1.1.3:\n",
            "      Successfully uninstalled datasets-1.1.3\n",
            "Successfully installed datasets-1.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ3IPlXrQ93X",
        "outputId": "264af3ae-ea83-4358-e249-313ee1527650"
      },
      "source": [
        "!python setup.py install"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 71kB 3.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 153kB 4.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 14.5MB 244kB/s \n",
            "\u001b[K     |████████████████████████████████| 748.8MB 23kB/s \n",
            "\u001b[K     |████████████████████████████████| 4.5MB 4.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 5.5MB/s \n",
            "\u001b[?25hrunning install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating deeplearning_nlp_models.egg-info\n",
            "writing deeplearning_nlp_models.egg-info/PKG-INFO\n",
            "writing dependency_links to deeplearning_nlp_models.egg-info/dependency_links.txt\n",
            "writing top-level names to deeplearning_nlp_models.egg-info/top_level.txt\n",
            "writing manifest file 'deeplearning_nlp_models.egg-info/SOURCES.txt'\n",
            "writing manifest file 'deeplearning_nlp_models.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/nlpmodels\n",
            "copying nlpmodels/__init__.py -> build/lib/nlpmodels\n",
            "creating build/lib/nlpmodels/utils\n",
            "copying nlpmodels/utils/label_smoother.py -> build/lib/nlpmodels/utils\n",
            "copying nlpmodels/utils/vocabulary.py -> build/lib/nlpmodels/utils\n",
            "copying nlpmodels/utils/gpt_sampler.py -> build/lib/nlpmodels/utils\n",
            "copying nlpmodels/utils/utils.py -> build/lib/nlpmodels/utils\n",
            "copying nlpmodels/utils/optims.py -> build/lib/nlpmodels/utils\n",
            "copying nlpmodels/utils/tokenizer.py -> build/lib/nlpmodels/utils\n",
            "copying nlpmodels/utils/train.py -> build/lib/nlpmodels/utils\n",
            "copying nlpmodels/utils/__init__.py -> build/lib/nlpmodels/utils\n",
            "creating build/lib/nlpmodels/models\n",
            "copying nlpmodels/models/word2vec.py -> build/lib/nlpmodels/models\n",
            "copying nlpmodels/models/transformer.py -> build/lib/nlpmodels/models\n",
            "copying nlpmodels/models/gpt.py -> build/lib/nlpmodels/models\n",
            "copying nlpmodels/models/text_cnn.py -> build/lib/nlpmodels/models\n",
            "copying nlpmodels/models/__init__.py -> build/lib/nlpmodels/models\n",
            "creating build/lib/nlpmodels/models/transformer_blocks\n",
            "copying nlpmodels/models/transformer_blocks/encoder.py -> build/lib/nlpmodels/models/transformer_blocks\n",
            "copying nlpmodels/models/transformer_blocks/decoder.py -> build/lib/nlpmodels/models/transformer_blocks\n",
            "copying nlpmodels/models/transformer_blocks/sublayers.py -> build/lib/nlpmodels/models/transformer_blocks\n",
            "copying nlpmodels/models/transformer_blocks/gpt_decoder.py -> build/lib/nlpmodels/models/transformer_blocks\n",
            "copying nlpmodels/models/transformer_blocks/attention.py -> build/lib/nlpmodels/models/transformer_blocks\n",
            "copying nlpmodels/models/transformer_blocks/__init__.py -> build/lib/nlpmodels/models/transformer_blocks\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/nlpmodels\n",
            "creating build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/label_smoother.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/vocabulary.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/gpt_sampler.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/utils.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/optims.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/tokenizer.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/train.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/__init__.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "creating build/bdist.linux-x86_64/egg/nlpmodels/models\n",
            "copying build/lib/nlpmodels/models/word2vec.py -> build/bdist.linux-x86_64/egg/nlpmodels/models\n",
            "copying build/lib/nlpmodels/models/transformer.py -> build/bdist.linux-x86_64/egg/nlpmodels/models\n",
            "copying build/lib/nlpmodels/models/gpt.py -> build/bdist.linux-x86_64/egg/nlpmodels/models\n",
            "creating build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer_blocks/encoder.py -> build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer_blocks/decoder.py -> build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer_blocks/sublayers.py -> build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer_blocks/gpt_decoder.py -> build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer_blocks/attention.py -> build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer_blocks/__init__.py -> build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/text_cnn.py -> build/bdist.linux-x86_64/egg/nlpmodels/models\n",
            "copying build/lib/nlpmodels/models/__init__.py -> build/bdist.linux-x86_64/egg/nlpmodels/models\n",
            "copying build/lib/nlpmodels/__init__.py -> build/bdist.linux-x86_64/egg/nlpmodels\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/label_smoother.py to label_smoother.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/vocabulary.py to vocabulary.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/gpt_sampler.py to gpt_sampler.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/utils.py to utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/optims.py to optims.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/tokenizer.py to tokenizer.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/train.py to train.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/word2vec.py to word2vec.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer.py to transformer.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/gpt.py to gpt.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks/encoder.py to encoder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks/decoder.py to decoder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks/sublayers.py to sublayers.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks/gpt_decoder.py to gpt_decoder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks/attention.py to attention.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/text_cnn.py to text_cnn.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/__init__.py to __init__.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying deeplearning_nlp_models.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying deeplearning_nlp_models.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying deeplearning_nlp_models.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying deeplearning_nlp_models.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/deeplearning_nlp_models-1.0-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing deeplearning_nlp_models-1.0-py3.6.egg\n",
            "Copying deeplearning_nlp_models-1.0-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding deeplearning-nlp-models 1.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/deeplearning_nlp_models-1.0-py3.6.egg\n",
            "Processing dependencies for deeplearning-nlp-models==1.0\n",
            "Finished processing dependencies for deeplearning-nlp-models==1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXZn0d9wiw4S"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPl37AC7Q7Ge"
      },
      "source": [
        "Here are the packages we need to import."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "R1xTK4WDQZi5"
      },
      "source": [
        "from nlpmodels.models import word2vec\n",
        "from nlpmodels.utils import utils, train\n",
        "from nlpmodels.utils.elt import skipgram_dataset\n",
        "from argparse import Namespace\n",
        "import torch\n",
        "utils.set_seed_everywhere()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "_FAdhDejQZi6"
      },
      "source": [
        "## Hyper-parameters\n",
        "\n",
        "These are the data processing, skip-gram, and model training hyper-parameters for this run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% Parameters\n"
        },
        "id": "IjnfEI2EQZi6"
      },
      "source": [
        "args = Namespace(\n",
        "    # skip gram data hyper-parameters\n",
        "    context_window_size = 5,\n",
        "    subsample_t = 10.e-5, # param for sub-sampling frequent words (10.e-5 suggested by paper)\n",
        "    # Model hyper-parameters\n",
        "    embedding_size = 512,\n",
        "    negative_sample_size= 20, # k examples to be used in negative sampling loss function\n",
        "    # Training hyper-parameters\n",
        "    num_epochs=50,\n",
        "    learning_rate=0.0001,\n",
        "    batch_size = 4096,\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "p_RwySKWQZi6"
      },
      "source": [
        "## Get Data\n",
        "\n",
        "Call the function that grabs training data (via hugging faces) and a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% Get data\n"
        },
        "id": "F6YRdoMSQZi6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13d6dba9-c09f-43f8-833d-dead12a67c69"
      },
      "source": [
        "train_dataloader, vocab = skipgram_dataset.SkipGramDataset.get_training_dataloader(args.context_window_size,\n",
        "                                                                                   args.subsample_t,\n",
        "                                                                                   args.batch_size)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% Variables\n"
        },
        "id": "NefAdy_3QZi7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59b96a35-ebe7-449c-f613-2bfd0f5c9acc"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "\n",
        "print(f\"The gist: context_window_size = {args.context_window_size}, \"\n",
        "      f\"batch_size = {args.batch_size}, vocab_size = {vocab_size}, \"\n",
        "      f\"embedding_size = {args.embedding_size}, k = {args.negative_sample_size}, \"\n",
        "      f\"train_size = {len(train_dataloader.dataset)}\"\n",
        "      )"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The gist: context_window_size = 5, batch_size = 4096, vocab_size = 61811, embedding_size = 512, k = 20, train_size = 16103933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "fCIfZnhkQZi7"
      },
      "source": [
        "## Training\n",
        "\n",
        "Here we build the model and call the trainer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% Setting up training the model\n"
        },
        "id": "Dr8YY7JiQZi7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "38407e05-128f-40ba-8c2d-5fe888c0d271"
      },
      "source": [
        "word_frequencies = torch.from_numpy(vocab.get_word_frequencies())\n",
        "model = word2vec.SkipGramNSModel(vocab_size, args.embedding_size, args.negative_sample_size,word_frequencies)\n",
        "trainer = train.Word2VecTrainer(args,model,train_dataloader)\n",
        "trainer.run()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 0]: 100%|██████████| 3932/3932 [03:04<00:00, 21.26it/s, loss=0.888]\n",
            "[Epoch 1]: 100%|██████████| 3932/3932 [03:00<00:00, 21.73it/s, loss=0.732]\n",
            "[Epoch 2]: 100%|██████████| 3932/3932 [03:00<00:00, 21.84it/s, loss=0.71]\n",
            "[Epoch 3]: 100%|██████████| 3932/3932 [02:59<00:00, 21.91it/s, loss=0.705]\n",
            "[Epoch 4]: 100%|██████████| 3932/3932 [02:59<00:00, 21.96it/s, loss=0.703]\n",
            "[Epoch 5]: 100%|██████████| 3932/3932 [03:01<00:00, 21.71it/s, loss=0.703]\n",
            "[Epoch 6]: 100%|██████████| 3932/3932 [03:01<00:00, 21.61it/s, loss=0.703]\n",
            "[Epoch 7]: 100%|██████████| 3932/3932 [03:01<00:00, 21.72it/s, loss=0.703]\n",
            "[Epoch 8]: 100%|██████████| 3932/3932 [03:00<00:00, 21.75it/s, loss=0.702]\n",
            "[Epoch 9]: 100%|██████████| 3932/3932 [03:00<00:00, 21.84it/s, loss=0.702]\n",
            "[Epoch 10]: 100%|██████████| 3932/3932 [02:59<00:00, 21.86it/s, loss=0.702]\n",
            "[Epoch 11]: 100%|██████████| 3932/3932 [02:59<00:00, 21.88it/s, loss=0.702]\n",
            "[Epoch 12]:  12%|█▏        | 459/3932 [00:23<02:29, 23.23it/s, loss=0.702]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-76388b671613>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSkipGramNSModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative_sample_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_frequencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2VecTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/deeplearning-nlp-models/nlpmodels/utils/train.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                     \u001b[0;31m# status bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                     \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;31m# debug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "zw_Ee0hbQZi7"
      },
      "source": [
        "## Examine Similarity of Embeddings\n",
        "\n",
        "Now that we've trained our embeddings, let's see if the words that are clustered together make any sense.\n",
        "\n",
        "We will use cosine similarity to find the embeddings that are most similar in the embeddings space. This is one metric\n",
        "for similarity. Another popular metric is based on euclidean distance. To use that metric, check out pytorch's\n",
        "cdist() function. Also, can't speak highly enough of `spotify::annoy` package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% Get the embeddings\n"
        },
        "id": "x-_W3wOrQZi7"
      },
      "source": [
        "embeddings = model.get_embeddings().to(torch.device('cpu'))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfcqWd-tVuBn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee7b4240-e8e4-4b7c-b6af-74dfbc11087b"
      },
      "source": [
        "embeddings"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3.3240e-04, -1.8160e-04,  8.7463e-05,  ...,  4.8631e-05,\n",
              "         -2.8457e-04, -6.9950e-04],\n",
              "        [-1.0018e-06,  7.1809e-04, -7.2560e-04,  ..., -8.2127e-04,\n",
              "         -3.2868e-04, -9.1119e-05],\n",
              "        [-5.3387e-04, -2.3233e-04,  7.4901e-04,  ...,  4.8751e-04,\n",
              "         -6.4552e-04, -1.1948e-04],\n",
              "        ...,\n",
              "        [ 6.9722e-02, -6.8210e-02,  6.9298e-02,  ...,  6.8851e-02,\n",
              "          6.9208e-02, -6.6931e-02],\n",
              "        [ 7.6255e-02, -7.4938e-02,  7.5374e-02,  ...,  7.5360e-02,\n",
              "          7.5257e-02, -7.4191e-02],\n",
              "        [ 8.0253e-02, -7.9104e-02,  7.8547e-02,  ...,  7.9173e-02,\n",
              "          8.0599e-02, -7.5644e-02]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "UY7Edm_vQZi7"
      },
      "source": [
        "### Computer\n",
        "\n",
        "Let's see the top 5 words associated with \"computer\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% computer similar words\n"
        },
        "id": "BSOnHyWBQZi7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86c08ac2-0609-424e-9031-34c55e42955c"
      },
      "source": [
        "utils.get_cosine_similar(\"computer\",vocab._token_to_idx,embeddings)[0:10]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r[Epoch 12]:  12%|█▏        | 459/3932 [00:40<02:29, 23.23it/s, loss=0.702]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('management', tensor(0.9933)),\n",
              " ('based', tensor(0.9907)),\n",
              " ('software', tensor(0.9903)),\n",
              " ('phone', tensor(0.9903)),\n",
              " ('services', tensor(0.9897)),\n",
              " ('systems', tensor(0.9864)),\n",
              " ('technology', tensor(0.9864)),\n",
              " ('business', tensor(0.9854)),\n",
              " ('personal', tensor(0.9849)),\n",
              " ('devices', tensor(0.9846))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "1dBWUJm5QZi7"
      },
      "source": [
        "### Market\n",
        "\n",
        "Let's see the top 5 words associated with \"market\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% market similar words\n"
        },
        "id": "JoKo6JySQZi7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ce543be-439c-4aef-c344-fdd85f67d341"
      },
      "source": [
        "utils.get_cosine_similar(\"market\",vocab._token_to_idx,embeddings)[0:10]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('exchange', tensor(0.9840)),\n",
              " ('cost', tensor(0.9811)),\n",
              " ('store', tensor(0.9786)),\n",
              " ('awaited', tensor(0.9784)),\n",
              " ('initial', tensor(0.9769)),\n",
              " ('industry', tensor(0.9766)),\n",
              " ('stock', tensor(0.9761)),\n",
              " ('auction', tensor(0.9752)),\n",
              " ('sector', tensor(0.9751)),\n",
              " ('slashed', tensor(0.9734))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "FqB5GFUfQZi8"
      },
      "source": [
        "In this particular example, we sub-selected heavily so that our training set would be manageable.\n",
        "With a training_N = ~200k and vocab_size = ~60k, we might consider increasing  N >> p to improve our embeddings."
      ]
    }
  ]
}