{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "TuFB1te9QZi4"
      },
      "source": [
        "# Skip-gram in Action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "8xfHObtRQZi5"
      },
      "source": [
        "## Colab Setup\n",
        "\n",
        "You can skip this section if not running on Google's colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KATZhWEKdvCt"
      },
      "source": [
        "If running with GPUs, sanity check that the GPUs are enabled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbMGzCvnhLoB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6141da3-c810-4e6f-bec7-3f1610098851"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Nov 30 02:08:31 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8    11W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ4c_IP5dt6K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35ea652a-8ccf-4003-e18c-eda469b4265d"
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blhzRQd4icDp"
      },
      "source": [
        "The above should be True. If not, debug (Note: version of pytorch I used is not capatible with CUDA drivers on colab. Follow these instructions here explicitly)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DYyFNKVQsuU"
      },
      "source": [
        "First, if running from colab, you must install the package. (You may skip if you installed already)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7_3_DdKQwsI",
        "outputId": "84c1e025-4e70-43ea-f4d8-c6e00a536700"
      },
      "source": [
        "!git clone --single-branch --branch colab https://github.com/will-thompson-k/deeplearning-nlp-models.git\n",
        "%cd deeplearning-nlp-models"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deeplearning-nlp-models'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/73)\u001b[K\rremote: Counting objects:   2% (2/73)\u001b[K\rremote: Counting objects:   4% (3/73)\u001b[K\rremote: Counting objects:   5% (4/73)\u001b[K\rremote: Counting objects:   6% (5/73)\u001b[K\rremote: Counting objects:   8% (6/73)\u001b[K\rremote: Counting objects:   9% (7/73)\u001b[K\rremote: Counting objects:  10% (8/73)\u001b[K\rremote: Counting objects:  12% (9/73)\u001b[K\rremote: Counting objects:  13% (10/73)\u001b[K\rremote: Counting objects:  15% (11/73)\u001b[K\rremote: Counting objects:  16% (12/73)\u001b[K\rremote: Counting objects:  17% (13/73)\u001b[K\rremote: Counting objects:  19% (14/73)\u001b[K\rremote: Counting objects:  20% (15/73)\u001b[K\rremote: Counting objects:  21% (16/73)\u001b[K\rremote: Counting objects:  23% (17/73)\u001b[K\rremote: Counting objects:  24% (18/73)\u001b[K\rremote: Counting objects:  26% (19/73)\u001b[K\rremote: Counting objects:  27% (20/73)\u001b[K\rremote: Counting objects:  28% (21/73)\u001b[K\rremote: Counting objects:  30% (22/73)\u001b[K\rremote: Counting objects:  31% (23/73)\u001b[K\rremote: Counting objects:  32% (24/73)\u001b[K\rremote: Counting objects:  34% (25/73)\u001b[K\rremote: Counting objects:  35% (26/73)\u001b[K\rremote: Counting objects:  36% (27/73)\u001b[K\rremote: Counting objects:  38% (28/73)\u001b[K\rremote: Counting objects:  39% (29/73)\u001b[K\rremote: Counting objects:  41% (30/73)\u001b[K\rremote: Counting objects:  42% (31/73)\u001b[K\rremote: Counting objects:  43% (32/73)\u001b[K\rremote: Counting objects:  45% (33/73)\u001b[K\rremote: Counting objects:  46% (34/73)\u001b[K\rremote: Counting objects:  47% (35/73)\u001b[K\rremote: Counting objects:  49% (36/73)\u001b[K\rremote: Counting objects:  50% (37/73)\u001b[K\rremote: Counting objects:  52% (38/73)\u001b[K\rremote: Counting objects:  53% (39/73)\u001b[K\rremote: Counting objects:  54% (40/73)\u001b[K\rremote: Counting objects:  56% (41/73)\u001b[K\rremote: Counting objects:  57% (42/73)\u001b[K\rremote: Counting objects:  58% (43/73)\u001b[K\rremote: Counting objects:  60% (44/73)\u001b[K\rremote: Counting objects:  61% (45/73)\u001b[K\rremote: Counting objects:  63% (46/73)\u001b[K\rremote: Counting objects:  64% (47/73)\u001b[K\rremote: Counting objects:  65% (48/73)\u001b[K\rremote: Counting objects:  67% (49/73)\u001b[K\rremote: Counting objects:  68% (50/73)\u001b[K\rremote: Counting objects:  69% (51/73)\u001b[K\rremote: Counting objects:  71% (52/73)\u001b[K\rremote: Counting objects:  72% (53/73)\u001b[K\rremote: Counting objects:  73% (54/73)\u001b[K\rremote: Counting objects:  75% (55/73)\u001b[K\rremote: Counting objects:  76% (56/73)\u001b[K\rremote: Counting objects:  78% (57/73)\u001b[K\rremote: Counting objects:  79% (58/73)\u001b[K\rremote: Counting objects:  80% (59/73)\u001b[K\rremote: Counting objects:  82% (60/73)\u001b[K\rremote: Counting objects:  83% (61/73)\u001b[K\rremote: Counting objects:  84% (62/73)\u001b[K\rremote: Counting objects:  86% (63/73)\u001b[K\rremote: Counting objects:  87% (64/73)\u001b[K\rremote: Counting objects:  89% (65/73)\u001b[K\rremote: Counting objects:  90% (66/73)\u001b[K\rremote: Counting objects:  91% (67/73)\u001b[K\rremote: Counting objects:  93% (68/73)\u001b[K\rremote: Counting objects:  94% (69/73)\u001b[K\rremote: Counting objects:  95% (70/73)\u001b[K\rremote: Counting objects:  97% (71/73)\u001b[K\rremote: Counting objects:  98% (72/73)\u001b[K\rremote: Counting objects: 100% (73/73)\u001b[K\rremote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 899 (delta 38), reused 31 (delta 15), pack-reused 826\u001b[K\n",
            "Receiving objects: 100% (899/899), 3.63 MiB | 40.36 MiB/s, done.\n",
            "Resolving deltas: 100% (530/530), done.\n",
            "/content/deeplearning-nlp-models/deeplearning-nlp-models/deeplearning-nlp-models/deeplearning-nlp-models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgWIaSV7SJ-y",
        "outputId": "6796a10c-976c-4f2e-9627-c5588290e668"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.6/dist-packages (1.1.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.4)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.18.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ3IPlXrQ93X",
        "outputId": "fc29b747-e3f3-4597-f936-2ca6b3713256"
      },
      "source": [
        "!python setup.py install"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating deeplearning_nlp_models.egg-info\n",
            "writing deeplearning_nlp_models.egg-info/PKG-INFO\n",
            "writing dependency_links to deeplearning_nlp_models.egg-info/dependency_links.txt\n",
            "writing top-level names to deeplearning_nlp_models.egg-info/top_level.txt\n",
            "writing manifest file 'deeplearning_nlp_models.egg-info/SOURCES.txt'\n",
            "writing manifest file 'deeplearning_nlp_models.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/nlpmodels\n",
            "copying nlpmodels/__init__.py -> build/lib/nlpmodels\n",
            "creating build/lib/nlpmodels/utils\n",
            "copying nlpmodels/utils/gpt_sampler.py -> build/lib/nlpmodels/utils\n",
            "copying nlpmodels/utils/label_smoother.py -> build/lib/nlpmodels/utils\n",
            "copying nlpmodels/utils/tokenizer.py -> build/lib/nlpmodels/utils\n",
            "copying nlpmodels/utils/__init__.py -> build/lib/nlpmodels/utils\n",
            "copying nlpmodels/utils/train.py -> build/lib/nlpmodels/utils\n",
            "copying nlpmodels/utils/vocabulary.py -> build/lib/nlpmodels/utils\n",
            "copying nlpmodels/utils/utils.py -> build/lib/nlpmodels/utils\n",
            "copying nlpmodels/utils/optims.py -> build/lib/nlpmodels/utils\n",
            "creating build/lib/nlpmodels/models\n",
            "copying nlpmodels/models/transformer.py -> build/lib/nlpmodels/models\n",
            "copying nlpmodels/models/__init__.py -> build/lib/nlpmodels/models\n",
            "copying nlpmodels/models/gpt.py -> build/lib/nlpmodels/models\n",
            "copying nlpmodels/models/text_cnn.py -> build/lib/nlpmodels/models\n",
            "copying nlpmodels/models/word2vec.py -> build/lib/nlpmodels/models\n",
            "creating build/lib/nlpmodels/models/transformer_blocks\n",
            "copying nlpmodels/models/transformer_blocks/gpt_decoder.py -> build/lib/nlpmodels/models/transformer_blocks\n",
            "copying nlpmodels/models/transformer_blocks/decoder.py -> build/lib/nlpmodels/models/transformer_blocks\n",
            "copying nlpmodels/models/transformer_blocks/encoder.py -> build/lib/nlpmodels/models/transformer_blocks\n",
            "copying nlpmodels/models/transformer_blocks/attention.py -> build/lib/nlpmodels/models/transformer_blocks\n",
            "copying nlpmodels/models/transformer_blocks/sublayers.py -> build/lib/nlpmodels/models/transformer_blocks\n",
            "copying nlpmodels/models/transformer_blocks/__init__.py -> build/lib/nlpmodels/models/transformer_blocks\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/nlpmodels\n",
            "creating build/bdist.linux-x86_64/egg/nlpmodels/models\n",
            "creating build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer_blocks/gpt_decoder.py -> build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer_blocks/decoder.py -> build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer_blocks/encoder.py -> build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer_blocks/attention.py -> build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer_blocks/sublayers.py -> build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer_blocks/__init__.py -> build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks\n",
            "copying build/lib/nlpmodels/models/transformer.py -> build/bdist.linux-x86_64/egg/nlpmodels/models\n",
            "copying build/lib/nlpmodels/models/__init__.py -> build/bdist.linux-x86_64/egg/nlpmodels/models\n",
            "copying build/lib/nlpmodels/models/gpt.py -> build/bdist.linux-x86_64/egg/nlpmodels/models\n",
            "copying build/lib/nlpmodels/models/text_cnn.py -> build/bdist.linux-x86_64/egg/nlpmodels/models\n",
            "copying build/lib/nlpmodels/models/word2vec.py -> build/bdist.linux-x86_64/egg/nlpmodels/models\n",
            "creating build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/gpt_sampler.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/label_smoother.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/tokenizer.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/__init__.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/train.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/vocabulary.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/utils.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/utils/optims.py -> build/bdist.linux-x86_64/egg/nlpmodels/utils\n",
            "copying build/lib/nlpmodels/__init__.py -> build/bdist.linux-x86_64/egg/nlpmodels\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks/gpt_decoder.py to gpt_decoder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks/decoder.py to decoder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks/encoder.py to encoder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks/attention.py to attention.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks/sublayers.py to sublayers.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer_blocks/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/transformer.py to transformer.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/gpt.py to gpt.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/text_cnn.py to text_cnn.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/models/word2vec.py to word2vec.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/gpt_sampler.py to gpt_sampler.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/label_smoother.py to label_smoother.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/tokenizer.py to tokenizer.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/train.py to train.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/vocabulary.py to vocabulary.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/utils.py to utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/utils/optims.py to optims.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nlpmodels/__init__.py to __init__.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying deeplearning_nlp_models.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying deeplearning_nlp_models.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying deeplearning_nlp_models.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying deeplearning_nlp_models.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/deeplearning_nlp_models-1.0-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing deeplearning_nlp_models-1.0-py3.6.egg\n",
            "Removing /usr/local/lib/python3.6/dist-packages/deeplearning_nlp_models-1.0-py3.6.egg\n",
            "Copying deeplearning_nlp_models-1.0-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "deeplearning-nlp-models 1.0 is already the active version in easy-install.pth\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/deeplearning_nlp_models-1.0-py3.6.egg\n",
            "Processing dependencies for deeplearning-nlp-models==1.0\n",
            "Finished processing dependencies for deeplearning-nlp-models==1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXZn0d9wiw4S"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPl37AC7Q7Ge"
      },
      "source": [
        "Here are the packages we need to import."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "R1xTK4WDQZi5"
      },
      "source": [
        "from nlpmodels.models import word2vec\n",
        "from nlpmodels.utils import utils, train\n",
        "from nlpmodels.utils.elt import skipgram_dataset\n",
        "from argparse import Namespace\n",
        "import torch\n",
        "utils.set_seed_everywhere()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "_FAdhDejQZi6"
      },
      "source": [
        "## Hyper-parameters\n",
        "\n",
        "These are the data processing, skip-gram, and model training hyper-parameters for this run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% Parameters\n"
        },
        "id": "IjnfEI2EQZi6"
      },
      "source": [
        "args = Namespace(\n",
        "    # skip gram data hyper-parameters\n",
        "    context_window_size = 5,\n",
        "    subsample_t = 10.e-15, # param for sub-sampling frequent words (10.e-5 suggested by paper)\n",
        "    # Model hyper-parameters\n",
        "    embedding_size = 300,\n",
        "    negative_sample_size= 20, # k examples to be used in negative sampling loss function\n",
        "    # Training hyper-parameters\n",
        "    num_epochs=100,\n",
        "    learning_rate=0.001,\n",
        "    batch_size = 8192,\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "p_RwySKWQZi6"
      },
      "source": [
        "## Get Data\n",
        "\n",
        "Call the function that grabs training data (via hugging faces) and a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% Get data\n"
        },
        "id": "F6YRdoMSQZi6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7824d6f7-86ee-4e1d-f5db-ea9b9d2499d2"
      },
      "source": [
        "train_dataloader, vocab = skipgram_dataset.SkipGramDataset.get_training_dataloader(args.context_window_size,\n",
        "                                                                                   args.subsample_t,\n",
        "                                                                                   args.batch_size)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% Variables\n"
        },
        "id": "NefAdy_3QZi7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aab52832-b785-4d09-fda0-bcf0684c8602"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "\n",
        "print(f\"The gist: context_window_size = {args.context_window_size}, \"\n",
        "      f\"batch_size = {args.batch_size}, vocab_size = {vocab_size}, \"\n",
        "      f\"embedding_size = {args.embedding_size}, k = {args.negative_sample_size}, \"\n",
        "      f\"train_size = {len(train_dataloader.dataset)}\"\n",
        "      )"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The gist: context_window_size = 5, batch_size = 8192, vocab_size = 61811, embedding_size = 300, k = 20, train_size = 720038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "fCIfZnhkQZi7"
      },
      "source": [
        "## Training\n",
        "\n",
        "Here we build the model and call the trainer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% Setting up training the model\n"
        },
        "id": "Dr8YY7JiQZi7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "77962ac1-623e-4894-91ef-f2a8d29246ea"
      },
      "source": [
        "word_frequencies = torch.from_numpy(vocab.get_word_frequencies())\n",
        "model = word2vec.SkipGramNSModel(vocab_size, args.embedding_size, args.negative_sample_size,word_frequencies)\n",
        "trainer = train.Word2VecTrainer(args,model,train_dataloader)\n",
        "trainer.run()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/88 [00:00<?, ?it/s]\u001b[A\n",
            "[Epoch 0]:   0%|          | 0/88 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-76388b671613>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSkipGramNSModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative_sample_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_frequencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2VecTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/deeplearning-nlp-models/nlpmodels/utils/train.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;31m# step 2. forward_prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;31m# step 3. compute the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deeplearning-nlp-models/nlpmodels/models/word2vec.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# (3) Look-up negative context word embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# (batch_size, k) -> (batch_size, k, embedding_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mneg_output_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_sample_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# (4) Calculate loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m         return F.embedding(\n\u001b[1;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1850\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input, output and indices must be on the current device"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "zw_Ee0hbQZi7"
      },
      "source": [
        "## Examine Similarity of Embeddings\n",
        "\n",
        "Now that we've trained our embeddings, let's see if the words that are clustered together make any sense.\n",
        "\n",
        "We will use cosine similarity to find the embeddings that are most similar in the embeddings space. This is one metric\n",
        "for similarity. Another popular metric is based on euclidean distance. To use that metric, check out pytorch's\n",
        "cdist() function. Also, can't speak highly enough of `spotify::annoy` package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% Get the embeddings\n"
        },
        "id": "x-_W3wOrQZi7"
      },
      "source": [
        "embeddings = model.get_embeddings()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "UY7Edm_vQZi7"
      },
      "source": [
        "### Computer\n",
        "\n",
        "Let's see the top 5 words associated with \"computer\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% computer similar words\n"
        },
        "id": "BSOnHyWBQZi7",
        "outputId": "d176753f-d6f8-433c-827b-797158f4fd31"
      },
      "source": [
        "utils.get_cosine_similar(\"computer\",vocab._token_to_idx,embeddings)[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('of', tensor(1.0000)),\n",
              " ('apple', tensor(1.0000)),\n",
              " ('israel', tensor(1.0000)),\n",
              " ('leader', tensor(1.0000)),\n",
              " ('game', tensor(1.0000))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "1dBWUJm5QZi7"
      },
      "source": [
        "### Market\n",
        "\n",
        "Let's see the top 5 words associated with \"market\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% market similar words\n"
        },
        "id": "JoKo6JySQZi7",
        "outputId": "82169e22-2e3f-4e87-a6b1-2af843f536da"
      },
      "source": [
        "utils.get_cosine_similar(\"market\",vocab._token_to_idx,embeddings)[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('investors', tensor(1.0000)),\n",
              " ('korea', tensor(1.0000)),\n",
              " ('out', tensor(1.0000)),\n",
              " ('israel', tensor(1.0000)),\n",
              " ('china', tensor(1.0000))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "FqB5GFUfQZi8"
      },
      "source": [
        "In this particular example, we sub-selected heavily so that our training set would be manageable.\n",
        "With a training_N = ~200k and vocab_size = ~60k, we might consider increasing  N >> p to improve our embeddings."
      ]
    }
  ]
}