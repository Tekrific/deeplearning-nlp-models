{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "word2vec.ipynb",
   "provenance": []
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "TuFB1te9QZi4"
   },
   "source": [
    "# Skip-gram in Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "8xfHObtRQZi5"
   },
   "source": [
    "## Colab Setup\n",
    "\n",
    "You can skip this section if not running on Google's colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KATZhWEKdvCt"
   },
   "source": [
    "If running with GPUs, sanity check that the GPUs are enabled."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RbMGzCvnhLoB"
   },
   "source": [
    "!nvidia-smi"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XQ4c_IP5dt6K",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7554ca33-2213-4147-e529-4d4985ce81b9"
   },
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 1
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blhzRQd4icDp"
   },
   "source": [
    "Ahould be True. If not, debug (Note: version of pytorch I used is not capatible with CUDA drivers on colab. Follow these instructions here explicitly)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TomQVpu6d1Qe",
    "outputId": "17c65fcc-2221-42f9-cfa1-e730f6026b42"
   },
   "source": [
    "!pwd"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/content\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsWgkCzbd4x8"
   },
   "source": [
    "This should be \"/content\" on Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DYyFNKVQsuU"
   },
   "source": [
    "First, if running from colab, you must install the package. (You may skip if you installed already)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g7_3_DdKQwsI"
   },
   "source": [
    "!git clone --single-branch --branch colab https://github.com/will-thompson-k/deeplearning-nlp-models.git\n",
    "%cd deeplearning-nlp-models"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wgWIaSV7SJ-y"
   },
   "source": [
    "!pip install datasets"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eZ3IPlXrQ93X"
   },
   "source": [
    "!python setup.py install"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXZn0d9wiw4S"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPl37AC7Q7Ge"
   },
   "source": [
    "Here are the packages we need to import."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "R1xTK4WDQZi5"
   },
   "source": [
    "from nlpmodels.models import word2vec\n",
    "from nlpmodels.utils import utils, train\n",
    "from nlpmodels.utils.elt import skipgram_dataset\n",
    "from argparse import Namespace\n",
    "import torch\n",
    "utils.set_seed_everywhere()"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "_FAdhDejQZi6"
   },
   "source": [
    "## Hyper-parameters\n",
    "\n",
    "These are the data processing, skip-gram, and model training hyper-parameters for this run."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%% Parameters\n"
    },
    "id": "IjnfEI2EQZi6"
   },
   "source": [
    "args = Namespace(\n",
    "    # skip gram data hyper-parameters\n",
    "    context_window_size = 10,\n",
    "    subsample_t = 10.e-7, # param for sub-sampling frequent words (10.e-5 suggested by paper)\n",
    "    # Model hyper-parameters\n",
    "    embedding_size = 512,\n",
    "    negative_sample_size= 20, # k examples to be used in negative sampling loss function\n",
    "    # Training hyper-parameters\n",
    "    num_epochs=50,\n",
    "    learning_rate=0.0001,\n",
    "    batch_size = 4096,\n",
    ")"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "p_RwySKWQZi6"
   },
   "source": [
    "## Get Data\n",
    "\n",
    "Call the function that grabs training data (via hugging faces) and a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%% Get data\n"
    },
    "id": "F6YRdoMSQZi6"
   },
   "source": [
    "train_dataloader, vocab = skipgram_dataset.SkipGramDataset.get_training_dataloader(args.context_window_size,\n",
    "                                                                                   args.subsample_t,\n",
    "                                                                                   args.batch_size)"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%% Variables\n"
    },
    "id": "NefAdy_3QZi7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c8c2a16e-4dc9-44db-9a1c-4714ac0e130f"
   },
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"The gist: context_window_size = {args.context_window_size}, \"\n",
    "      f\"batch_size = {args.batch_size}, vocab_size = {vocab_size}, \"\n",
    "      f\"embedding_size = {args.embedding_size}, k = {args.negative_sample_size}, \"\n",
    "      f\"train_size = {len(train_dataloader.dataset)}\"\n",
    "      )"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "The gist: context_window_size = 10, batch_size = 4096, vocab_size = 61811, embedding_size = 512, k = 20, train_size = 2787170\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "fCIfZnhkQZi7"
   },
   "source": [
    "## Training\n",
    "\n",
    "Here we build the model and call the trainer."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%% Setting up training the model\n"
    },
    "id": "Dr8YY7JiQZi7"
   },
   "source": [
    "word_frequencies = torch.from_numpy(vocab.get_word_frequencies())\n",
    "model = word2vec.SkipGramNSModel(vocab_size, args.embedding_size, args.negative_sample_size,word_frequencies)\n",
    "trainer = train.Word2VecTrainer(args,model,train_dataloader)\n",
    "trainer.run()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "zw_Ee0hbQZi7"
   },
   "source": [
    "## Examine Similarity of Embeddings\n",
    "\n",
    "Now that we've trained our embeddings, let's see if the words that are clustered together make any sense.\n",
    "\n",
    "We will use cosine similarity to find the embeddings that are most similar in the embeddings space. This is one metric\n",
    "for similarity. Another popular metric is based on euclidean distance. To use that metric, check out pytorch's\n",
    "cdist() function. Also, can't speak highly enough of `spotify::annoy` package."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%% Get the embeddings\n"
    },
    "id": "x-_W3wOrQZi7"
   },
   "source": [
    "embeddings = model.get_embeddings().to(torch.device('cpu'))"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AcwxFtWmjV64",
    "outputId": "e9431113-20ad-4665-c652-2f096eb0c8df"
   },
   "source": [
    "embeddings"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 3.3240e-04, -1.8160e-04,  8.7463e-05,  ...,  4.8631e-05,\n",
       "         -2.8457e-04, -6.9950e-04],\n",
       "        [-1.0018e-06,  7.1809e-04, -7.2560e-04,  ..., -8.2127e-04,\n",
       "         -3.2868e-04, -9.1119e-05],\n",
       "        [-5.3387e-04, -2.3233e-04,  7.4901e-04,  ...,  4.8751e-04,\n",
       "         -6.4552e-04, -1.1948e-04],\n",
       "        ...,\n",
       "        [ 7.5621e-04,  3.8125e-04,  3.1393e-04,  ...,  7.3202e-04,\n",
       "          5.5612e-04,  3.6417e-04],\n",
       "        [ 8.0143e-02, -7.9639e-02,  7.9455e-02,  ...,  7.9445e-02,\n",
       "          7.9570e-02, -8.0183e-02],\n",
       "        [ 8.9229e-02, -8.9240e-02,  8.8531e-02,  ...,  8.8135e-02,\n",
       "          8.8876e-02, -8.9196e-02]])"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 13
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "UY7Edm_vQZi7"
   },
   "source": [
    "### Computer\n",
    "\n",
    "Let's see the top 10 words associated with \"computer\"."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%% computer similar words\n"
    },
    "id": "BSOnHyWBQZi7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a9ca8fb9-45ef-4674-d58b-fd2ddbf0c762"
   },
   "source": [
    "utils.get_cosine_similar(\"computer\",vocab._token_to_idx,embeddings)[0:10]"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('systems', tensor(0.9948)),\n",
       " ('phone', tensor(0.9944)),\n",
       " ('online', tensor(0.9920)),\n",
       " ('ipod', tensor(0.9917)),\n",
       " ('digital', tensor(0.9907)),\n",
       " ('server', tensor(0.9885)),\n",
       " ('product', tensor(0.9883)),\n",
       " ('storage', tensor(0.9869)),\n",
       " ('services', tensor(0.9860)),\n",
       " ('technology', tensor(0.9856))]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 14
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "1dBWUJm5QZi7"
   },
   "source": [
    "### Market\n",
    "\n",
    "Let's see the top 10 words associated with \"market\"."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%% market similar words\n"
    },
    "id": "JoKo6JySQZi7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7f18ee25-3fdb-4ce9-bb04-2964ad962d08"
   },
   "source": [
    "utils.get_cosine_similar(\"market\",vocab._token_to_idx,embeddings)[0:10]"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('expectations', tensor(0.9880)),\n",
       " ('income', tensor(0.9880)),\n",
       " ('nasdaq', tensor(0.9869)),\n",
       " ('analysts', tensor(0.9869)),\n",
       " ('october', tensor(0.9856)),\n",
       " ('raised', tensor(0.9843)),\n",
       " ('awaited', tensor(0.9839)),\n",
       " ('low', tensor(0.9835)),\n",
       " ('financial', tensor(0.9830)),\n",
       " ('global', tensor(0.9824))]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 15
    }
   ]
  }
 ]
}