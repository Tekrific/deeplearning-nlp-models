{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CNN-based Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Colab Setup\n",
    "\n",
    "You can skip this section if not running on Google's colab."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If running with GPUs, sanity check that the GPUs are enabled."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Should be True. If not, debug (Note: version of pytorch I used is not capatible with CUDA drivers on colab. Follow these instructions here explicitly)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pwd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This should be \"/content\" on Colab."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, if running from colab, you must install the package. (You may skip if you installed already)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone --single-branch --branch colab https://github.com/will-thompson-k/deeplearning-nlp-models.git\n",
    "%cd deeplearning-nlp-models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install datasets torchtext==0.7.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python setup.py install"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports\n",
    "\n",
    "Here are the packages we need to import."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from nlpmodels.models import text_cnn\n",
    "from nlpmodels.utils import train,utils\n",
    "from nlpmodels.utils.elt import text_cnn_dataset\n",
    "from argparse import Namespace\n",
    "utils.set_seed_everywhere()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentiment Analysis with CNNs\n",
    "\n",
    "Following the logic in Kim's paper, we are running an embedding + convolutional layer architecture in order\n",
    "to conduct sentiment analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyper-parameters\n",
    "\n",
    "These are the data processing and model training hyper-parameters for this run."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "        # Model hyper-parameters\n",
    "        max_sequence_length=50, #Often you chose it such that there is minimal padding. 95th percentile=582\n",
    "        dim_model=128, # Embedding size controls the projection of a vocabulary.\n",
    "        num_filters=128, # output filters from convolution\n",
    "        window_sizes=[3,5,7], # different filter sizes, total number of filters len(window_sizes)*num_filters\n",
    "        num_classes=2, # binary classification problem\n",
    "        dropout=0.5, # 0.5 from original implementation, kind of high compared to other papers (usually 0.1)\n",
    "        # Training hyper-parameters\n",
    "        num_epochs=30,\n",
    "        learning_rate=1.e-6, #chosing LR is important, often accompanied with scheduler to change\n",
    "        batch_size=64\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Parameters\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25000lines [00:20, 1227.02lines/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-cebd298a2ca4>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# pydev_debug_cell\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvocab\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtext_cnn_dataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTextCNNDataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_training_dataloader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m model = text_cnn.TextCNN(vocab_size = len(vocab),\n\u001B[1;32m      4\u001B[0m                         \u001B[0mdim_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdim_model\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m                         \u001B[0mnum_filters\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnum_filters\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/codes/will-thompson-k/deeplearning-nlp-models/nlpmodels/utils/elt/text_cnn_dataset.py\u001B[0m in \u001B[0;36mget_training_dataloader\u001B[0;34m(cls, args)\u001B[0m\n\u001B[1;32m     62\u001B[0m         \u001B[0mbatch_size\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m         \u001B[0mmax_sequence_length\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax_sequence_length\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 64\u001B[0;31m         \u001B[0mtrain_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvocab\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_training_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmax_sequence_length\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     65\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m         train_loader = DataLoader(train_data,\n",
      "\u001B[0;32m~/Documents/codes/will-thompson-k/deeplearning-nlp-models/nlpmodels/utils/elt/text_cnn_dataset.py\u001B[0m in \u001B[0;36mget_training_data\u001B[0;34m(cls, max_sequence_length)\u001B[0m\n\u001B[1;32m     85\u001B[0m         \u001B[0mtrain_dataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mIMDB\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_select\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'train'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     86\u001B[0m         \u001B[0;31m# note: targets are {0,1} and the data is not shuffled\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 87\u001B[0;31m         \u001B[0mtrain_target\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_text\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mtrain_dataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     88\u001B[0m         \u001B[0;31m# tokenize the data using our tokenizer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     89\u001B[0m         \u001B[0mtrain_text\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtokenize_corpus_basic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_text\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/codes/will-thompson-k/deeplearning-nlp-models/nlpmodels/utils/elt/text_cnn_dataset.py\u001B[0m in \u001B[0;36mget_training_data\u001B[0;34m(cls, max_sequence_length)\u001B[0m\n\u001B[1;32m     85\u001B[0m         \u001B[0mtrain_dataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mIMDB\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_select\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'train'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     86\u001B[0m         \u001B[0;31m# note: targets are {0,1} and the data is not shuffled\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 87\u001B[0;31m         \u001B[0mtrain_target\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_text\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mtrain_dataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     88\u001B[0m         \u001B[0;31m# tokenize the data using our tokenizer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     89\u001B[0m         \u001B[0mtrain_text\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtokenize_corpus_basic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_text\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_36_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_36_64.ThreadTracer.__call__\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_36_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_36_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_36_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_36_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_36_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_36_64.PyDBFrame.do_wait_suspend\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py\u001B[0m in \u001B[0;36mdo_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1101\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1102\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_threads_suspended_single_notification\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnotify_thread_suspended\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop_reason\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1103\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_do_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1104\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1105\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_do_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py\u001B[0m in \u001B[0;36m_do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1116\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1117\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprocess_internal_commands\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1118\u001B[0;31m                 \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0.01\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1119\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1120\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcancel_async_evaluation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mget_current_thread_id\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train_loader, vocab = text_cnn_dataset.TextCNNDataset.get_training_dataloader(args)\n",
    "model = text_cnn.TextCNN(vocab_size = len(vocab),\n",
    "                        dim_model = args.dim_model,\n",
    "                        num_filters = args.num_filters,\n",
    "                        window_sizes =  args.window_sizes,\n",
    "                        num_classes = args.num_classes,\n",
    "                        dropout = args.dropout)\n",
    "\n",
    "trainer = train.TextCNNTrainer(args, vocab.mask_index, model, train_loader, vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's run this."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 0]: 100%|██████████| 391/391 [01:26<00:00,  4.53it/s, accuracy=50, loss=15.2]  \n",
      "[Epoch 1]: 100%|██████████| 391/391 [01:24<00:00,  4.64it/s, accuracy=50, loss=14.4]  \n",
      "[Epoch 2]: 100%|██████████| 391/391 [01:19<00:00,  4.90it/s, accuracy=50, loss=13.6]  \n",
      "[Epoch 3]: 100%|██████████| 391/391 [01:20<00:00,  4.85it/s, accuracy=50.1, loss=15.7]\n",
      "[Epoch 4]: 100%|██████████| 391/391 [01:21<00:00,  4.78it/s, accuracy=50.3, loss=14.1]\n",
      "[Epoch 5]: 100%|██████████| 391/391 [01:44<00:00,  3.75it/s, accuracy=50.2, loss=12.6]\n",
      "[Epoch 6]:  27%|██▋       | 105/391 [00:30<01:38,  2.91it/s, accuracy=13.6, loss=23.1]"
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Review\n",
    "\n",
    "The goal is just to show how this works - you can play with the hyper-parameters as you see fit.\n",
    "In an ideal situation, we would check the data against an unseen val or test set to diagnose performance.\n",
    "\n",
    "#### Parameter importance\n",
    "\n",
    "In playing with the model, there are a few things to note:\n",
    "\n",
    "- *l2 regularization*: Unlike the original paper, I didn't end up using L2 regularization.\n",
    "- *dictionary pruning*: The original dictionary had 75k tokens. I ended up pruning any <.1% frequency, bringing it down\n",
    "to <20k.\n",
    "- *max_sequence_length*: Generally, you don't want to truncate the sentences and want to set this to the longest sequence.\n",
    "However, here the max == ~2k while the 95th percentile was ~500, so I chose to truncate some sentences.\n",
    "- *learning_rate*: I set the parameter to be static,\n",
    "but often times it makes sense to use a scheduler to allow larger parameter changes initially and then fine-tune over updates.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}