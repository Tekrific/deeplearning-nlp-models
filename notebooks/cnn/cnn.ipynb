{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "cnn.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "B6NTfhMQMhix"
      },
      "source": [
        "# CNN-based Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "7VZSYMzaMhix"
      },
      "source": [
        "## Colab Setup\n",
        "\n",
        "You can skip this section if not running on Google's colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "x0EmYcLMMhix"
      },
      "source": [
        "If running with GPUs, sanity check that the GPUs are enabled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "L4JVqFu5Mhix"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "hvenhtLuMhix",
        "outputId": "42e1fdb8-99f2-453c-bb5d-64239ebd5a34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "tVdsCtGsMhix"
      },
      "source": [
        "Should be True. If not, debug (Note: version of pytorch I used is not capatible with CUDA drivers on colab. Follow these instructions here explicitly)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "nlHDPsPmMhix"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "-1vymbroMhix"
      },
      "source": [
        "This should be \"/content\" on Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "WIHQuG87Mhix"
      },
      "source": [
        "First, if running from colab, you must install the package. (You may skip if you installed already)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "lSqoFD0hMhix"
      },
      "source": [
        "!git clone --single-branch --branch colab https://github.com/will-thompson-k/deeplearning-nlp-models.git\n",
        "%cd deeplearning-nlp-models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bnr6nyMTMhix"
      },
      "source": [
        "!pip install datasets torchtext==0.8.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "YbrgD9L0Mhix"
      },
      "source": [
        "!python setup.py install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "PRnpA4AdMhiy"
      },
      "source": [
        "## Imports\n",
        "\n",
        "Here are the packages we need to import."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "CA8xNzhwMhiy"
      },
      "source": [
        "from nlpmodels.models import text_cnn\n",
        "from nlpmodels.utils import train,utils\n",
        "from nlpmodels.utils.elt import text_cnn_dataset\n",
        "from argparse import Namespace\n",
        "utils.set_seed_everywhere()\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "deAoUKmxMhiy"
      },
      "source": [
        "## Sentiment Analysis with CNNs\n",
        "\n",
        "Following the logic in Kim's paper, we are running an embedding + convolutional layer architecture in order\n",
        "to conduct sentiment analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "PNg8Z752Mhiy"
      },
      "source": [
        "### Hyper-parameters\n",
        "\n",
        "These are the data processing and model training hyper-parameters for this run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% Parameters\n"
        },
        "id": "vUkBYQuOMhiy"
      },
      "source": [
        "args = Namespace(\n",
        "        # Model hyper-parameters\n",
        "        max_sequence_length=50, #Often you chose it such that there is minimal padding. 95th percentile=582\n",
        "        dim_model=128, # Embedding size controls the projection of a vocabulary.\n",
        "        num_filters=128, # output filters from convolution\n",
        "        window_sizes=[3,5,7], # different filter sizes, total number of filters len(window_sizes)*num_filters\n",
        "        num_classes=2, # binary classification problem\n",
        "        dropout=0.5, # 0.5 from original implementation, kind of high compared to other papers (usually 0.1)\n",
        "        # Training hyper-parameters\n",
        "        num_epochs=30,\n",
        "        learning_rate=1.e-6, #chosing LR is important, often accompanied with scheduler to change\n",
        "        batch_size=64\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bidyCWgWMhiy"
      },
      "source": [
        "train_loader, vocab = text_cnn_dataset.TextCNNDataset.get_training_dataloader(args)\n",
        "model = text_cnn.TextCNN(vocab_size = len(vocab),\n",
        "                        dim_model = args.dim_model,\n",
        "                        num_filters = args.num_filters,\n",
        "                        window_sizes =  args.window_sizes,\n",
        "                        num_classes = args.num_classes,\n",
        "                        dropout = args.dropout)\n",
        "\n",
        "trainer = train.TextCNNTrainer(args, vocab.mask_index, model, train_loader, vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "izOpAMT5Mhiz"
      },
      "source": [
        "Let's run this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "EtC1p6r3Mhiz"
      },
      "source": [
        "trainer.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "_rGsDesxMhiz"
      },
      "source": [
        "### Review\n",
        "\n",
        "The goal is just to show how this works - you can play with the hyper-parameters as you see fit.\n",
        "In an ideal situation, we would check the data against an unseen val or test set to diagnose performance.\n",
        "\n",
        "#### Parameter importance\n",
        "\n",
        "In playing with the model, there are a few things to note:\n",
        "\n",
        "- *l2 regularization*: Unlike the original paper, I didn't end up using L2 regularization.\n",
        "- *dictionary pruning*: The original dictionary had 75k tokens. I ended up pruning any <.1% frequency, bringing it down\n",
        "to <20k.\n",
        "- *max_sequence_length*: Generally, you don't want to truncate the sentences and want to set this to the longest sequence.\n",
        "However, here the max == ~2k while the 95th percentile was ~500, so I chose to truncate some sentences.\n",
        "- *learning_rate*: I set the parameter to be static,\n",
        "but often times it makes sense to use a scheduler to allow larger parameter changes initially and then fine-tune over updates.\n",
        "\n"
      ]
    }
  ]
}