{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# The GPT Language Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports\n",
    "\n",
    "Here are the packages we need to import."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from nlpmodels.models import gpt\n",
    "from nlpmodels.utils import train,utils,gpt_dataset,gpt_sampler\n",
    "from argparse import Namespace\n",
    "utils.set_seed_everywhere()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Language Model: WikiText2\n",
    "\n",
    "We will try to train our transformer model to learn how to predict the next word in torchtext WikiText2 database.\n",
    "I took the first 300k from the training set to reduce computation time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyper-parameters\n",
    "\n",
    "These are the data processing and model training hyper-parameters for this run. Note that we are running a smaller model\n",
    "than cited in the paper for fewer iterations...on a CPU. This is meant merely to demonstrate it works."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "        # Model hyper-parameters\n",
    "        num_layers_per_stack=2,  # original value = 12\n",
    "        dim_model=12, #original value = 768\n",
    "        dim_ffn=48, # original value = 3072\n",
    "        num_heads=2, # original value = 12\n",
    "        block_size=64, # original value = 512, context window\n",
    "        dropout=0.1,\n",
    "        # Training hyper-parameters\n",
    "        num_epochs=2, #obviously super short\n",
    "        learning_rate=0.0,\n",
    "        batch_size=32, #original value = 64\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Parameters\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1lines [00:00,  3.40lines/s]\n"
     ]
    }
   ],
   "source": [
    "train_loader, vocab = gpt_dataset.GPTDataset.get_training_dataloader(args)\n",
    "model = gpt.GPT(vocab_size = len(vocab),\n",
    "            num_layers_per_stack= args.num_layers_per_stack,\n",
    "            dim_model = args.dim_model,\n",
    "            dim_ffn = args.dim_ffn,\n",
    "            num_heads = args.num_heads,\n",
    "            block_size = args.block_size,\n",
    "            dropout = args.dropout)\n",
    "trainer = train.GPTTrainer(args,vocab.mask_index,model,train_loader,vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we will run the first step in GPT training process, where we train the model to\n",
    "maximize the objective\n",
    "\n",
    "```\n",
    "max p(x[k]|x[k-1],[k-2],...x[k-block_size])\n",
    "```.\n",
    "\n",
    "This is an unsupervised (more aptly described as \"self-supervised\") loss. After this model is trained,\n",
    "we can run then continue it onto another problem (can freeze layers to only continue training the top layers)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 0]:   0%|          | 10/9375 [00:12<3:08:29,  1.21s/it, loss=10.4]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-041e2033e90a>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/Documents/codes/will-thompson-k/deeplearning-nlp-models/nlpmodels/utils/train.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    196\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    197\u001B[0m                 \u001B[0;31m# step 3. compute the standard cross entropy loss\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 198\u001B[0;31m                 \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_loss_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_hat\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mview\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_hat\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtgt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mview\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    199\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# step 4. back_prop\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda/envs/p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    720\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    721\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 722\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    723\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    724\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda/envs/p36/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input, target)\u001B[0m\n\u001B[1;32m    946\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    947\u001B[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001B[0;32m--> 948\u001B[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001B[0m\u001B[1;32m    949\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    950\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda/envs/p36/lib/python3.6/site-packages/torch/nn/functional.py\u001B[0m in \u001B[0;36mcross_entropy\u001B[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001B[0m\n\u001B[1;32m   2420\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0msize_average\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mreduce\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2421\u001B[0m         \u001B[0mreduction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_Reduction\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlegacy_get_string\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msize_average\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreduce\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2422\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mnll_loss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlog_softmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mignore_index\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreduction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2423\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2424\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda/envs/p36/lib/python3.6/site-packages/torch/nn/functional.py\u001B[0m in \u001B[0;36mlog_softmax\u001B[0;34m(input, dim, _stacklevel, dtype)\u001B[0m\n\u001B[1;32m   1589\u001B[0m         \u001B[0mdim\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_get_softmax_dim\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'log_softmax'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdim\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_stacklevel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1590\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mdtype\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1591\u001B[0;31m         \u001B[0mret\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlog_softmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1592\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1593\u001B[0m         \u001B[0mret\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlog_softmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdim\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GPT Completes A Sequence\n",
    "\n",
    "In the spirit of Kaparthy's minGPT::play_char notebook, we can use a greedy_sampler to see how the model\n",
    "continues a sequence."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk> stringent iguana diaries stylistic erzherzog stroke erase designations fireballs clean hamilton charms ain check publicized minginish reiterating roamed township microscopically grocery nefer minarsih vidal itself laz multitude masters nut subsistence respite indisputably ancient usace 1879 imp kirkus sebastian army maturation beirut technical middle mccombe aluminum 122 fangs protracted consul maryada portion proprietors slipped angles documenting speeches whether treknation harm an carson purchased nearest persons\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "prompt = \"Super Mario Land is a 1989 side @-@ scrolling platform video game , \" \\\n",
    "         \"the first in the Super Mario Land series \" \\\n",
    "         \", developed and published by Nintendo as a launch title for their Game Boy \" \\\n",
    "         \"handheld game console . In gameplay similar to that of the 1985 Super Mario Bros.\" \\\n",
    "         \" , but resized for the smaller device 's screen , the player advances Mario to the \" \\\n",
    "         \"end of 12 levels by moving to the right and jumping across platforms to avoid enemies\" \\\n",
    "         \" and pitfalls . \"\n",
    "prompt_tensor = torch.LongTensor([[vocab.lookup_token(s) for s in prompt.split(\" \")]])\n",
    "prompt_tensor_batch = trainer._reformat_data((prompt_tensor,None))\n",
    "steps = 64\n",
    "yhat_indices = gpt_sampler.greedy_sampler(model=model, x=prompt_tensor_batch, steps=steps,block_size=64,do_sample=True).src\n",
    "yhat_tokens = ' '.join([vocab.lookup_index(int(idx)) for idx in yhat_indices[0]])\n",
    "print(yhat_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once the model is trained in the self-supervised phase, go forth and apply it to a different problem."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}