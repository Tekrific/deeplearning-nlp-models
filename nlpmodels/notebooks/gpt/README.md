# GPT: Unsupervised Pre-training & the Decoder-only Transformer

This is the implementation of OpenAI's style of Transformer, the "Generative Pre-training" (i.e. GPT) model, which has 3 generations:
1) <ins>GPT-1</ins>: Radford et. al ["Improving Language Understanding by Generative Pre-Training"](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) (2018).
2) <ins>GPT-2</ins>: Radford et. al ["Language Models are Unsupervised Multitask Learners"](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (2019).
3) <ins>GPT-3</ins>: Brown et. al ["Language Models are Few-Shot Learners"](https://arxiv.org/pdf/2005.14165.pdf) (2020).

## Contents

- [Jupyter Notebook](#Notebook)
- [Code](#Code)
- [Example Usage](#Usage)
- [Background](#Background)
- [GPT Overview](#GPT)
- [GPT-1](##GPT-1)
- [GPT-2](##GPT-2)
- [GPT-3](##GPT-3)
- [Features](#Features)
- [References](#References)
- [Citation](#Citation)
- [License](#License)