{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Skip-gram in Action"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports\n",
    "\n",
    "Here are the packages we need to import."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from nlpmodels.models import word2vec\n",
    "from nlpmodels.utils import skipgram_dataset,utils\n",
    "from nlpmodels.utils import train\n",
    "from argparse import Namespace\n",
    "import torch\n",
    "utils.set_seed_everywhere()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyper-parameters\n",
    "\n",
    "These are the data processing, skip-gram, and model training hyper-parameters for this run."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # skip gram data hyper-parameters\n",
    "    context_window_size = 5,\n",
    "    subsample_t = 10.e-15, # param for sub-sampling frequent words (10.e-5 suggested by paper)\n",
    "    # Model hyper-parameters\n",
    "    embedding_size = 300,\n",
    "    negative_sample_size= 20, # k examples to be used in negative sampling loss function\n",
    "    # Training hyper-parameters\n",
    "    num_epochs=100,\n",
    "    learning_rate=0.0001,\n",
    "    batch_size = 4096,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Parameters\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get Data\n",
    "\n",
    "Call the function that grabs training data (via hugging faces) and a dictionary."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%% Get data\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, vocab = skipgram_dataset.SkipGramDataset.get_training_dataloader(args.context_window_size,\n",
    "                                                                                   args.subsample_t,\n",
    "                                                                                   args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gist: context_window_size = 5, batch_size = 4096, vocab_size = 61810, embedding_size = 300, k = 20, train_size = 240032\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"The gist: context_window_size = {args.context_window_size}, \"\n",
    "      f\"batch_size = {args.batch_size}, vocab_size = {vocab_size}, \"\n",
    "      f\"embedding_size = {args.embedding_size}, k = {args.negative_sample_size}, \"\n",
    "      f\"train_size = {len(train_dataloader.dataset)}\"\n",
    "      )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Variables\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "\n",
    "Here we build the model and call the trainer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 0]: 100%|██████████| 59/59 [01:04<00:00,  1.09s/it, loss=14.5]\n",
      "[Epoch 99]: 100%|██████████| 59/59 [00:50<00:00,  1.16it/s, loss=0.744]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training...\n"
     ]
    }
   ],
   "source": [
    "word_frequencies = torch.from_numpy(vocab.get_word_frequencies())\n",
    "model = word2vec.SkipGramNSModel(vocab_size, args.embedding_size, args.negative_sample_size,word_frequencies)\n",
    "trainer = train.Word2VecTrainer(args,model,train_dataloader)\n",
    "trainer.run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Setting up training the model\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Examine Similarity of Embeddings\n",
    "\n",
    "Now that we've trained our embeddings, let's see if the words that are clustered together make any sense."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "embeddings = model.get_embeddings()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Get the embeddings\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Computer\n",
    "\n",
    "Let's see the top 5 words associated with \"computer\"."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "[('of', tensor(1.0000)),\n ('apple', tensor(1.0000)),\n ('israel', tensor(1.0000)),\n ('leader', tensor(1.0000)),\n ('game', tensor(1.0000))]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.get_cosine_similar(\"computer\",vocab._token_to_idx,embeddings)[0:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% computer similar words\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Market\n",
    "\n",
    "Let's see the top 5 words associated with \"market\"."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "[('investors', tensor(1.0000)),\n ('korea', tensor(1.0000)),\n ('out', tensor(1.0000)),\n ('israel', tensor(1.0000)),\n ('china', tensor(1.0000))]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.get_cosine_similar(\"market\",vocab._token_to_idx,embeddings)[0:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% market similar words\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this particular example, we sub-selected heavily so that our training set would be manageable.\n",
    "With a training_N = ~200k and vocab_size = ~60k, we might consider increasing  N >> p to improve our embeddings."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}